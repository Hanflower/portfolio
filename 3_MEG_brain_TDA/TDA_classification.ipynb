{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0442e987",
   "metadata": {},
   "source": [
    "# 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c112cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "import gtda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75d8ee",
   "metadata": {},
   "source": [
    "# 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"C:\\\\*****\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85e6e3",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433eadec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 3)\n",
      "(90, 3)\n",
      "(90, 3)\n",
      "(90, 3)\n",
      "(90, 3)\n",
      "(90, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pe_h0</th>\n",
       "      <th>pe_h1</th>\n",
       "      <th>pe_h2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.055588</td>\n",
       "      <td>3.226400</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.060383</td>\n",
       "      <td>3.569570</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.091665</td>\n",
       "      <td>3.689771</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.071445</td>\n",
       "      <td>2.739480</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.066084</td>\n",
       "      <td>1.999732</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pe_h0     pe_h1  pe_h2\n",
       "0  6.055588  3.226400   -1.0\n",
       "1  6.060383  3.569570   -1.0\n",
       "2  6.091665  3.689771   -1.0\n",
       "3  6.071445  2.739480   -1.0\n",
       "4  6.066084  1.999732   -1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_time1 = pd.read_csv(\"G:\\\\*****\\\\@@@@@.csv\", names=[\"pe_h0\", \"pe_h1\", \"pe_h2\"], skiprows=1)\n",
    "\n",
    "print(pe_time1.shape)\n",
    "print(pe_time2.shape)\n",
    "print(pe_time3.shape)\n",
    "print(pe_time4.shape)\n",
    "print(pe_time5.shape)\n",
    "print(pe_time6.shape)\n",
    "pe_time1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be356b1",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a766f503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all time for persistence entropy h`\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"persistence entropy at time 1\") \n",
    "plot_data = [pe_time1['pe_h1'].values[:22], pe_time1['pe_h1'].values[23:45], pe_time1['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 2) \n",
    "plt.title(\"persistence entropy at time 2\")\n",
    "plot_data = [pe_time2['pe_h1'].values[:22], pe_time2['pe_h1'].values[23:45], pe_time2['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 3) \n",
    "plt.title(\"persistence entropy at time 3\")\n",
    "plot_data = [pe_time3['pe_h1'].values[:22], pe_time3['pe_h1'].values[23:45], pe_time3['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"persistence entropy at time 4\") \n",
    "plot_data = [pe_time4['pe_h1'].values[:22], pe_time4['pe_h1'].values[23:45], pe_time4['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 5) \n",
    "plt.title(\"persistence entropy at time 5\")\n",
    "plot_data = [pe_time5['pe_h1'].values[:22], pe_time5['pe_h1'].values[23:45], pe_time5['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 6) \n",
    "plt.title(\"persistence entropy at time 6\")\n",
    "plot_data = [pe_time6['pe_h1'].values[:22], pe_time6['pe_h1'].values[23:45], pe_time6['pe_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b17eb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all time for number of points h1\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"number of points h1 argmax at time 1\") \n",
    "plot_data = [np_time1['np_h1'].values[:22], np_time1['np_h1'].values[23:45], np_time1['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 2) \n",
    "plt.title(\"number of points h1 argmax at time 2\")\n",
    "plot_data = [np_time2['np_h1'].values[:22], np_time2['np_h1'].values[23:45], np_time2['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 3) \n",
    "plt.title(\"number of points h1 argmax at time 3\")\n",
    "plot_data = [np_time3['np_h1'].values[:22], np_time3['np_h1'].values[23:45], np_time3['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"number of points h1 argmax at time 4\") \n",
    "plot_data = [np_time4['np_h1'].values[:22], np_time4['np_h1'].values[23:45], np_time4['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 5) \n",
    "plt.title(\"number of points h1 argmax at time 5\")\n",
    "plot_data = [np_time5['np_h1'].values[:22], np_time5['np_h1'].values[23:45], np_time5['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 6) \n",
    "plt.title(\"number of points h1 argmax at time 6\")\n",
    "plot_data = [np_time6['np_h1'].values[:22], np_time6['np_h1'].values[23:45], np_time6['np_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36339372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all time for persistence landscape h1\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"persistence landscape h1 argmax at time 1\") \n",
    "plot_data = [pl_time1['pl_h1'].values[:22], pl_time1['pl_h1'].values[23:45], pl_time1['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 2) \n",
    "plt.title(\"persistence landscape h1 argmax at time 2\")\n",
    "plot_data = [pl_time2['pl_h1'].values[:22], pl_time2['pl_h1'].values[23:45], pl_time2['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 3) \n",
    "plt.title(\"persistence landscape h1 argmax at time 3\")\n",
    "plot_data = [pl_time3['pl_h1'].values[:22], pl_time3['pl_h1'].values[23:45], pl_time3['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"persistence landscape h1 argmax at time 4\") \n",
    "plot_data = [pl_time4['pl_h1'].values[:22], pl_time4['pl_h1'].values[23:45], pl_time4['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 5) \n",
    "plt.title(\"persistence landscape h1 argmax at time 5\")\n",
    "plot_data = [pl_time5['pl_h1'].values[:22], pl_time5['pl_h1'].values[23:45], pl_time5['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 6) \n",
    "plt.title(\"persistence landscape h1 argmax at time 6\")\n",
    "plot_data = [pl_time6['pl_h1'].values[:22], pl_time6['pl_h1'].values[23:45], pl_time6['pl_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32428317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all time for betti curve h1\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"betti curve h1 argmax at time 1\") \n",
    "plot_data = [curve_time1['curve_h1'].values[:22], curve_time1['curve_h1'].values[23:45], curve_time1['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 2) \n",
    "plt.title(\"betti curve h1 argmax at time 2\")\n",
    "plot_data = [curve_time2['curve_h1'].values[:22], curve_time2['curve_h1'].values[23:45], curve_time2['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 3) \n",
    "plt.title(\"betti curve h1 argmax at time 3\")\n",
    "plot_data = [curve_time3['curve_h1'].values[:22], curve_time3['curve_h1'].values[23:45], curve_time3['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"betti curve h1 argmax at time 4\") \n",
    "plot_data = [curve_time4['curve_h1'].values[:22], curve_time4['curve_h1'].values[23:45], curve_time4['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 5) \n",
    "plt.title(\"betti curve h1 argmax at time 5\")\n",
    "plot_data = [curve_time5['curve_h1'].values[:22], curve_time5['curve_h1'].values[23:45], curve_time5['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.subplot(2, 3, 6) \n",
    "plt.title(\"betti curve h1 argmax at time 6\")\n",
    "plot_data = [curve_time6['curve_h1'].values[:22], curve_time6['curve_h1'].values[23:45], curve_time6['curve_h1'].values[46:]]\n",
    "plt.boxplot(plot_data)\n",
    "plt.xticks([1, 2, 3], ['LT', 'RT', 'HC'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123ec16",
   "metadata": {},
   "source": [
    "# 모델 A (betti curve argmax만 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde4433",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2429c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curve_h1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   curve_h1\n",
       "0        49\n",
       "1        49\n",
       "2        49\n",
       "3        48\n",
       "4        49"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time1 = curve_time1[[\"curve_h1\"]]\n",
    "df_time2 = curve_time2[[\"curve_h1\"]]\n",
    "df_time3 = curve_time3[[\"curve_h1\"]]\n",
    "df_time4 = curve_time4[[\"curve_h1\"]]\n",
    "df_time5 = curve_time5[[\"curve_h1\"]]\n",
    "df_time6 = curve_time6[[\"curve_h1\"]]\n",
    "\n",
    "print(df_time1.shape)\n",
    "print(df_time2.shape)\n",
    "print(df_time3.shape)\n",
    "print(df_time4.shape)\n",
    "print(df_time5.shape)\n",
    "print(df_time6.shape)\n",
    "df_time1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec5b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_time1\n",
      "pl_h1       61\n",
      "curve_h1    44\n",
      "dtype: int64\n",
      "pl_h1       79\n",
      "curve_h1    60\n",
      "dtype: int64\n",
      "df_time2\n",
      "pl_h1       65\n",
      "curve_h1    46\n",
      "dtype: int64\n",
      "pl_h1       78\n",
      "curve_h1    59\n",
      "dtype: int64\n",
      "df_time3\n",
      "pl_h1       62\n",
      "curve_h1    33\n",
      "dtype: int64\n",
      "pl_h1       79\n",
      "curve_h1    61\n",
      "dtype: int64\n",
      "df_time4\n",
      "pl_h1       59\n",
      "curve_h1    31\n",
      "dtype: int64\n",
      "pl_h1       80\n",
      "curve_h1    61\n",
      "dtype: int64\n",
      "df_time5\n",
      "pl_h1       0\n",
      "curve_h1    0\n",
      "dtype: int64\n",
      "pl_h1       80\n",
      "curve_h1    62\n",
      "dtype: int64\n",
      "df_time6\n",
      "pl_h1       0\n",
      "curve_h1    0\n",
      "dtype: int64\n",
      "pl_h1       79\n",
      "curve_h1    62\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for embedding\n",
    "print(\"df_time1\")\n",
    "print(df_time1.min())\n",
    "print(df_time1.max())\n",
    "print(\"df_time2\")\n",
    "print(df_time2.min())\n",
    "print(df_time2.max())\n",
    "print(\"df_time3\")\n",
    "print(df_time3.min())\n",
    "print(df_time3.max())\n",
    "print(\"df_time4\")\n",
    "print(df_time4.min())\n",
    "print(df_time4.max())\n",
    "print(\"df_time5\")\n",
    "print(df_time5.min())\n",
    "print(df_time5.max())\n",
    "print(\"df_time6\")\n",
    "print(df_time6.min())\n",
    "print(df_time6.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954efc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdd68e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_time1, df_time2, df_time3, df_time4, df_time5, df_time6]\n",
    "x = [] \n",
    "\n",
    "for i in range(90) :\n",
    "    sample_x = [] \n",
    "    for j in range(6) :\n",
    "        sample_x.append(dfs[j].iloc[i])    \n",
    "    x.append(sample_x)\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74269868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 0 : lt, 1 : rt, 2 : hc\n",
    "y = np.array(np.repeat([0,1,2], [22, 22, 46], axis=0)) \n",
    "y = y.reshape(90,-1)\n",
    "\n",
    "# 정답 데이터 one-hot vector 처리\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64adb7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 6, 1)\n",
      "(90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# (90, 6, feature 수)\n",
    "# (90, group 수(=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42efe1",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42170b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Dropout, Flatten, Embedding, TimeDistributed, Reshape, Concatenate\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3da003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embednet(shape): # shape=3\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=81, output_dim=2, input_length=shape))\n",
    "    model.add(Reshape(target_shape=(-1,)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a04e2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k겹 때마다 생성할 모델 \n",
    "def model_fn(feature_num) :\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding\n",
    "    model.add(TimeDistributed(build_embednet(feature_num), input_shape=(6, feature_num))) \n",
    "\n",
    "    # GRU (Sequence)\n",
    "    model.add(GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu', return_sequences=True))\n",
    "    model.add(GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu')) # , return_sequences=True\n",
    "    \n",
    "    # Classification\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(3, activation='softmax')) # final classification\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c415977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 6, 2)             162       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 6, 8)              288       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 8)                 432       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                144       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,349\n",
      "Trainable params: 1,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn(1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdaff85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "391b9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k겹 때마다 가장 좋은 모델을 저장하기 위한 파일 이름\n",
    "def get_model_name(k):\n",
    "    return '/best_model/model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "890c3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09554, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09554 to 1.09271, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09271 to 1.08988, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08988 to 1.08720, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08720 to 1.08443, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08443 to 1.08238, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08238 to 1.08036, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08036 to 1.07852, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07852 to 1.07699, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07699 to 1.07578, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07578 to 1.07477, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.07477 to 1.07414, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.07414 to 1.07365, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.07365 to 1.07347, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.07347\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.07347\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.07347\n",
      "\n",
      "Epoch 18: val_loss improved from 1.07347 to 1.07339, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.07339 to 1.07250, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.07250 to 1.07188, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.07188 to 1.07096, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.07096 to 1.06994, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.06994 to 1.06779, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.06779 to 1.06464, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.06464 to 1.06074, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.06074 to 1.05572, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.05572 to 1.04927, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 1.04927 to 1.04199, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 1.04199 to 1.03175, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 1.03175 to 1.01967, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 1.01967 to 1.00549, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 1.00549 to 0.99102, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.99102 to 0.97379, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.97379 to 0.95498, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.95498 to 0.93164, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.93164 to 0.90726, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.90726 to 0.88040, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.88040 to 0.85364, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.85364 to 0.82679, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.82679 to 0.79848, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.79848 to 0.76892, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.76892 to 0.73858, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.73858 to 0.71264, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.71264 to 0.68791, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.68791 to 0.66442, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.66442 to 0.64493, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.64493 to 0.63257, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.63257 to 0.62081, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.62081 to 0.60493, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.60493 to 0.58744, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.58744 to 0.56740, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.56740 to 0.55395, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.55395 to 0.54995, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.54995 to 0.54829, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.54829\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.54829\n",
      "\n",
      "Epoch 57: val_loss improved from 0.54829 to 0.52683, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.52683 to 0.50246, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.50246 to 0.48399, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.48399 to 0.47211, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.47211 to 0.46280, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.46280 to 0.45452, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.45452 to 0.44909, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.44909 to 0.44406, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.44406\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.44406\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.44406\n",
      "\n",
      "Epoch 68: val_loss improved from 0.44406 to 0.42272, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.42272 to 0.40844, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.40844 to 0.39802, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.39802 to 0.38959, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.38959\n",
      "\n",
      "Epoch 73: val_loss improved from 0.38959 to 0.38857, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.38857 to 0.38036, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.38036 to 0.36997, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.36997 to 0.36217, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.36217\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.36217\n",
      "\n",
      "Epoch 79: val_loss improved from 0.36217 to 0.34360, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.34360 to 0.33955, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.33955\n",
      "\n",
      "Epoch 82: val_loss improved from 0.33955 to 0.32205, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.32205 to 0.30770, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.30770 to 0.30137, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.30137 to 0.29018, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.29018 to 0.28670, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.28670 to 0.27168, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.27168 to 0.26129, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.26129 to 0.24967, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.24967 to 0.24310, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.24310 to 0.23521, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.23521 to 0.21816, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.21816 to 0.20685, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.20685 to 0.19074, saving model to /best_model\\model_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95: val_loss improved from 0.19074 to 0.17743, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.17743 to 0.16329, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.16329 to 0.14907, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.14907 to 0.14115, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.14115 to 0.12435, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.12435 to 0.11390, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.11390 to 0.10478, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.10478 to 0.09245, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.09245 to 0.08932, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.08932 to 0.08284, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.08284 to 0.07261, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.07261 to 0.06348, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.06348 to 0.05723, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.05723 to 0.05229, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.05229 to 0.04701, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.04701 to 0.04230, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.04230 to 0.03765, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.03765 to 0.03409, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.03409\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.03409\n",
      "\n",
      "Epoch 115: val_loss improved from 0.03409 to 0.02989, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.02989 to 0.02239, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.02239 to 0.02082, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.02082 to 0.01961, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.01961 to 0.01861, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.01861\n",
      "\n",
      "Epoch 121: val_loss improved from 0.01861 to 0.01835, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.01835 to 0.01617, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01617\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01617 to 0.01440, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01440\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.01440\n",
      "\n",
      "Epoch 136: val_loss improved from 0.01440 to 0.01303, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01303 to 0.01274, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 143: val_loss improved from 0.01274 to 0.01087, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.01087 to 0.00910, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.00910 to 0.00836, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.00836 to 0.00805, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.00805 to 0.00773, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.00773 to 0.00734, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.00734 to 0.00698, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.00698 to 0.00676, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.00676 to 0.00658, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.00658 to 0.00649, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.00649 to 0.00618, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00618\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00618\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00618\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00618\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00618\n",
      "\n",
      "Epoch 159: val_loss improved from 0.00618 to 0.00530, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.00530 to 0.00497, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00497 to 0.00491, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00491 to 0.00477, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00477\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00477\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00477\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00477 to 0.00476, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00476 to 0.00462, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.00462 to 0.00433, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.00433 to 0.00409, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.00409 to 0.00407, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00407 to 0.00402, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00402 to 0.00387, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.00387 to 0.00373, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.00373 to 0.00336, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00336 to 0.00325, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00325 to 0.00312, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00312\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00312\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00312\n",
      "\n",
      "Epoch 180: val_loss improved from 0.00312 to 0.00306, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.00306 to 0.00298, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.00298 to 0.00292, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00292 to 0.00285, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.00285 to 0.00280, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00280 to 0.00269, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 192: val_loss improved from 0.00269 to 0.00234, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.00234 to 0.00216, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00216 to 0.00214, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00214 to 0.00203, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00203 to 0.00194, saving model to /best_model\\model_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 203: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00194 to 0.00186, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00186 to 0.00180, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.00180 to 0.00177, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.00177 to 0.00176, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00176 to 0.00175, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00175 to 0.00170, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00170\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00170\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00170\n",
      "\n",
      "Epoch 213: val_loss improved from 0.00170 to 0.00155, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.00155 to 0.00144, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.00144 to 0.00139, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.00139 to 0.00134, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.00134 to 0.00132, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00132\n",
      "\n",
      "Epoch 224: val_loss improved from 0.00132 to 0.00130, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.00130 to 0.00123, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.00123 to 0.00115, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.00115 to 0.00110, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.00110 to 0.00108, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 234: val_loss improved from 0.00108 to 0.00107, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.00107 to 0.00104, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 242: val_loss improved from 0.00104 to 0.00093, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.00093 to 0.00084, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.00084 to 0.00080, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.00080 to 0.00079, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.00079 to 0.00078, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.00078 to 0.00074, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.00074 to 0.00066, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.00066 to 0.00062, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00062\n",
      "\n",
      "Epoch 276: val_loss improved from 0.00062 to 0.00061, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.00061 to 0.00058, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.00058 to 0.00053, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.00053 to 0.00050, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.00050 to 0.00050, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00050\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.9837e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09521, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09521 to 1.09216, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09216 to 1.08961, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08961 to 1.08659, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08659 to 1.08356, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08356 to 1.08062, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08062 to 1.07736, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07736 to 1.07408, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07408 to 1.07012, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07012 to 1.06641, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06641 to 1.06274, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06274 to 1.05866, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05866 to 1.05481, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.05481 to 1.05120, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05120 to 1.04760, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.04760 to 1.04339, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.04339 to 1.03863, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.03863 to 1.03268, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.03268 to 1.02676, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.02676 to 1.02112, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 1.02112 to 1.01393, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.01393 to 1.00562, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.00562 to 0.99626, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.99626 to 0.98511, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.98511 to 0.97384, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.97384 to 0.96171, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.96171 to 0.94824, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.94824 to 0.93335, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.93335 to 0.91801, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.91801 to 0.90232, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.90232 to 0.88786, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.88786 to 0.87316, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.87316 to 0.85755, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.85755 to 0.83915, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.83915 to 0.82020, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.82020 to 0.80185, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.80185 to 0.78073, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.78073 to 0.76007, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.76007 to 0.73955, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.73955 to 0.72002, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.72002 to 0.70215, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.70215 to 0.68141, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.68141 to 0.66574, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.66574 to 0.65543, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.65543 to 0.64043, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.64043 to 0.62614, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.62614 to 0.61597, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.61597 to 0.60403, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.60403 to 0.59061, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.59061 to 0.57762, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.57762 to 0.56676, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.56676 to 0.55909, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.55909 to 0.54197, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.54197 to 0.52985, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.52985 to 0.51771, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.51771 to 0.50649, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.50649 to 0.49399, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.49399\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.49399\n",
      "\n",
      "Epoch 60: val_loss improved from 0.49399 to 0.47920, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.47920 to 0.43706, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.43706 to 0.42253, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.42253 to 0.40473, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.40473\n",
      "\n",
      "Epoch 65: val_loss improved from 0.40473 to 0.39606, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.39606 to 0.36317, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.36317 to 0.33928, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.33928 to 0.33278, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.33278 to 0.30204, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.30204 to 0.28049, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.28049 to 0.26518, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.26518 to 0.24964, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.24964 to 0.22320, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.22320 to 0.21913, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.21913 to 0.20361, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.20361 to 0.17500, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.17500\n",
      "\n",
      "Epoch 78: val_loss improved from 0.17500 to 0.17071, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.17071 to 0.14866, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.14866 to 0.13205, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.13205\n",
      "\n",
      "Epoch 82: val_loss improved from 0.13205 to 0.11231, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.11231 to 0.10549, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.10549 to 0.09861, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09861\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09861\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09861\n",
      "\n",
      "Epoch 88: val_loss improved from 0.09861 to 0.07525, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.07525 to 0.06773, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.06773\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.06773\n",
      "\n",
      "Epoch 92: val_loss improved from 0.06773 to 0.06268, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.06268 to 0.05580, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.05580 to 0.05292, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.05292 to 0.05149, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05149\n",
      "\n",
      "Epoch 107: val_loss improved from 0.05149 to 0.05065, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.05065 to 0.04647, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.04647 to 0.04490, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.04490 to 0.04270, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.04270 to 0.03913, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.03913 to 0.03687, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.03687 to 0.03629, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.03629\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.03629\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.03629\n",
      "\n",
      "Epoch 117: val_loss improved from 0.03629 to 0.03412, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.03412\n",
      "\n",
      "Epoch 119: val_loss improved from 0.03412 to 0.03396, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.03396 to 0.03257, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 121: val_loss did not improve from 0.03257\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.03257\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.03257\n",
      "\n",
      "Epoch 124: val_loss improved from 0.03257 to 0.03055, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.03055 to 0.02260, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.02260 to 0.01948, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.01948 to 0.01805, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.01805 to 0.01729, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.01729\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01729\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01729\n",
      "\n",
      "Epoch 132: val_loss improved from 0.01729 to 0.01429, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01429 to 0.01311, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.01311 to 0.01282, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.01282\n",
      "\n",
      "Epoch 136: val_loss improved from 0.01282 to 0.01244, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01244 to 0.01174, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01174\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.01174\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.01174\n",
      "\n",
      "Epoch 141: val_loss improved from 0.01174 to 0.01031, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.01031 to 0.00971, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00971\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00971 to 0.00951, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00951 to 0.00796, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00796\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00796\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00796\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00796 to 0.00691, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00691 to 0.00676, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00676 to 0.00619, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.00619 to 0.00588, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.00588 to 0.00562, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00562 to 0.00556, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00556 to 0.00552, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 220: val_loss improved from 0.00552 to 0.00550, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.00550 to 0.00532, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.00532 to 0.00526, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.00526 to 0.00524, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.00524 to 0.00514, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00514\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00514\n",
      "\n",
      "Epoch 227: val_loss improved from 0.00514 to 0.00492, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.00492 to 0.00468, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.00468 to 0.00455, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00455\n",
      "\n",
      "Epoch 249: val_loss improved from 0.00455 to 0.00448, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.00448 to 0.00426, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.00426 to 0.00409, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.00409 to 0.00399, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.00399 to 0.00390, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.00390 to 0.00384, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 255: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 258: val_loss improved from 0.00384 to 0.00380, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.00380 to 0.00378, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.00378 to 0.00374, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.00374 to 0.00362, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.00362 to 0.00352, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.00352 to 0.00343, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 290: val_loss improved from 0.00343 to 0.00327, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.00327 to 0.00295, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.00295 to 0.00273, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.00273 to 0.00256, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.00256 to 0.00247, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.00247 to 0.00240, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00240\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09732, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09732 to 1.09607, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09607 to 1.09459, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09459 to 1.09309, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09309 to 1.09174, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.09174 to 1.09042, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.09042 to 1.08893, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08893 to 1.08737, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08737 to 1.08566, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08566 to 1.08377, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.08377 to 1.08166, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08166 to 1.07927, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.07927 to 1.07655, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.07655 to 1.07353, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.07353 to 1.07016, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.07016 to 1.06615, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.06615 to 1.06157, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.06157 to 1.05651, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.05651 to 1.04986, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.04986 to 1.04159, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.04159 to 1.03287, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.03287 to 1.02258, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.02258 to 1.01122, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.01122 to 0.99832, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.99832 to 0.98335, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.98335 to 0.96677, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.96677 to 0.94834, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.94834 to 0.92729, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.92729 to 0.90241, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.90241 to 0.87431, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.87431 to 0.84364, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.84364 to 0.81345, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.81345 to 0.78337, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.78337 to 0.75360, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.75360 to 0.72482, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.72482 to 0.70040, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.70040 to 0.68081, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.68081 to 0.66396, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.66396 to 0.64469, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.64469 to 0.62818, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.62818 to 0.61791, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.61791 to 0.61149, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.61149 to 0.60563, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.60563 to 0.60146, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.60146 to 0.59525, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.59525\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.59525\n",
      "\n",
      "Epoch 48: val_loss improved from 0.59525 to 0.58397, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.58397 to 0.55568, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.55568 to 0.54901, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.54901 to 0.54336, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.54336 to 0.53727, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.53727 to 0.53103, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.53103 to 0.52476, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.52476 to 0.51895, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.51895 to 0.51357, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.51357 to 0.50710, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.50710 to 0.50056, saving model to /best_model\\model_3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss improved from 0.50056 to 0.49400, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.49400 to 0.48746, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.48746 to 0.48050, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.48050 to 0.47385, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.47385 to 0.46705, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.46705 to 0.46139, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.46139 to 0.45504, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.45504 to 0.44896, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.44896 to 0.44430, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.44430 to 0.43968, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.43968 to 0.43468, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.43468 to 0.42932, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.42932 to 0.42417, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.42417 to 0.42199, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.42199\n",
      "\n",
      "Epoch 74: val_loss improved from 0.42199 to 0.41955, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.41955 to 0.41676, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.41676 to 0.41403, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.41403 to 0.40823, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.40823 to 0.39320, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.39320\n",
      "\n",
      "Epoch 80: val_loss improved from 0.39320 to 0.38500, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.38500\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.38500\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.38500\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.38500\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.38500\n",
      "\n",
      "Epoch 86: val_loss improved from 0.38500 to 0.37555, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.37555 to 0.35537, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.35537\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.35537\n",
      "\n",
      "Epoch 90: val_loss improved from 0.35537 to 0.35467, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.35467 to 0.33584, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.33584 to 0.32997, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.32997 to 0.32216, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.32216 to 0.29786, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.29786 to 0.27372, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.27372 to 0.26160, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.26160 to 0.23882, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.23882 to 0.22584, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.22584 to 0.22476, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.22476 to 0.16937, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.16937 to 0.16881, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.16881 to 0.16387, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.16387 to 0.13388, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.13388 to 0.10631, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.10631\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.10631\n",
      "\n",
      "Epoch 107: val_loss improved from 0.10631 to 0.08715, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.08715 to 0.07040, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.07040 to 0.06791, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.06791 to 0.06545, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.06545 to 0.05853, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.05853 to 0.05409, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05409\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05409\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05409\n",
      "\n",
      "Epoch 116: val_loss improved from 0.05409 to 0.05292, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.05292 to 0.03744, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.03744 to 0.03395, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.03395 to 0.03020, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.03020 to 0.02592, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.02592 to 0.02589, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.02589\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.02589\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.02589\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.02589\n",
      "\n",
      "Epoch 126: val_loss improved from 0.02589 to 0.02292, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.02292 to 0.01995, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.01995 to 0.01964, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.01964 to 0.01825, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 140: val_loss improved from 0.01825 to 0.01338, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.01338 to 0.01176, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.01176 to 0.01170, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 149: val_loss improved from 0.01170 to 0.00739, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.00739 to 0.00622, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.00622 to 0.00621, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00621\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00621 to 0.00563, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00563\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00563 to 0.00457, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00457\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00457 to 0.00341, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.00341 to 0.00247, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.00247 to 0.00247, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00247\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00247\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09678, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09678 to 1.09472, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09472 to 1.09216, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09216 to 1.08945, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08945 to 1.08680, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08680 to 1.08420, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08420 to 1.08076, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08076 to 1.07713, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07713 to 1.07371, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07371 to 1.07037, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07037 to 1.06735, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06735 to 1.06446, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06446 to 1.06149, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06149 to 1.05879, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05879 to 1.05617, saving model to /best_model\\model_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: val_loss improved from 1.05617 to 1.05348, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05348 to 1.05041, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.05041 to 1.04692, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04692 to 1.04328, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.04328 to 1.04022, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.04022 to 1.03672, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.03672 to 1.03212, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.03212 to 1.02613, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.02613 to 1.01880, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.01880 to 1.01041, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.01041 to 1.00151, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.00151 to 0.99243, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.99243 to 0.98250, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.98250 to 0.97154, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.97154 to 0.95982, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.95982 to 0.94827, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.94827 to 0.93558, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.93558 to 0.92108, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.92108 to 0.90517, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.90517 to 0.88853, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.88853 to 0.87058, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.87058 to 0.85259, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.85259 to 0.83533, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.83533 to 0.81808, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.81808 to 0.80212, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.80212 to 0.78668, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.78668 to 0.77233, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.77233 to 0.75618, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.75618 to 0.74373, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.74373 to 0.73696, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.73696 to 0.72828, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.72828 to 0.71585, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.71585 to 0.70173, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.70173 to 0.69165, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.69165 to 0.68397, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.68397 to 0.67490, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.67490 to 0.66563, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.66563 to 0.65891, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.65891 to 0.65431, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.65431 to 0.64431, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.64431 to 0.63265, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.63265 to 0.62235, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.62235 to 0.61525, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.61525\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.61525\n",
      "\n",
      "Epoch 61: val_loss improved from 0.61525 to 0.61511, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.61511 to 0.59759, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.59759 to 0.58208, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.58208 to 0.57534, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.57534 to 0.57140, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.57140 to 0.56142, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.56142 to 0.55708, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.55708 to 0.55405, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.55405 to 0.54170, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.54170 to 0.52711, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.52711 to 0.51915, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.51915 to 0.50333, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.50333 to 0.47735, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.47735 to 0.45551, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.45551\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.45551\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.45551\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.45551\n",
      "\n",
      "Epoch 79: val_loss improved from 0.45551 to 0.44671, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.44671 to 0.42496, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.42496 to 0.39725, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.39725 to 0.37780, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.37780 to 0.36362, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.36362\n",
      "\n",
      "Epoch 85: val_loss improved from 0.36362 to 0.36121, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.36121 to 0.33332, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.33332 to 0.30803, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.30803 to 0.28937, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.28937 to 0.27072, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.27072 to 0.25888, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.25888 to 0.25178, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.25178\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.25178\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.25178\n",
      "\n",
      "Epoch 95: val_loss improved from 0.25178 to 0.24226, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.24226 to 0.22651, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.22651 to 0.21220, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.21220 to 0.19927, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.19927 to 0.19139, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.19139 to 0.18590, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.18590 to 0.18162, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.18162 to 0.17951, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.17951 to 0.17493, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.17493 to 0.16713, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.16713 to 0.16100, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.16100 to 0.15363, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.15363 to 0.14791, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.14791 to 0.14688, saving model to /best_model\\model_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 109: val_loss improved from 0.14688 to 0.13771, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.13771 to 0.13136, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.13136\n",
      "\n",
      "Epoch 112: val_loss improved from 0.13136 to 0.12869, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.12869 to 0.12091, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.12091 to 0.11387, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.11387 to 0.10764, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.10764\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.10764\n",
      "\n",
      "Epoch 118: val_loss improved from 0.10764 to 0.09537, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.09537 to 0.09015, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09015\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09015\n",
      "\n",
      "Epoch 122: val_loss improved from 0.09015 to 0.08881, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.08881 to 0.07816, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.07816 to 0.06802, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.06802 to 0.06203, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.06203 to 0.05794, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.05794 to 0.05410, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.05410 to 0.05065, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.05065 to 0.04796, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.04796 to 0.04424, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.04424 to 0.04096, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.04096 to 0.03885, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.03885 to 0.03793, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.03793 to 0.03672, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.03672 to 0.03585, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.03585\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.03585\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.03585\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.03585\n",
      "\n",
      "Epoch 140: val_loss improved from 0.03585 to 0.02803, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.02803 to 0.02209, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.02209 to 0.01995, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.01995 to 0.01867, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.01867 to 0.01752, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.01752 to 0.01654, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.01654 to 0.01565, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.01565 to 0.01405, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.01405 to 0.01296, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.01296 to 0.01224, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.01224 to 0.01143, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.01143 to 0.01040, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.01040 to 0.00958, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.00958 to 0.00910, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.00910 to 0.00873, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00873\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00873\n",
      "\n",
      "Epoch 157: val_loss improved from 0.00873 to 0.00863, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00863\n",
      "\n",
      "Epoch 159: val_loss improved from 0.00863 to 0.00844, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.00844 to 0.00746, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00746 to 0.00722, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00722 to 0.00662, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.00662 to 0.00628, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00628 to 0.00614, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00614\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00614\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00614 to 0.00592, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.00592 to 0.00577, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.00577 to 0.00487, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.00487 to 0.00420, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00420 to 0.00398, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00398 to 0.00376, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00376 to 0.00327, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00327\n",
      "\n",
      "Epoch 188: val_loss improved from 0.00327 to 0.00315, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.00315 to 0.00310, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00310 to 0.00304, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00304\n",
      "\n",
      "Epoch 192: val_loss improved from 0.00304 to 0.00295, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.00295 to 0.00281, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00281 to 0.00265, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.00265 to 0.00249, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00249 to 0.00236, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.00236 to 0.00223, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.00223 to 0.00210, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.00210 to 0.00200, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.00200 to 0.00190, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.00190 to 0.00178, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00178 to 0.00167, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.00167 to 0.00159, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00159 to 0.00153, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00153 to 0.00153, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00153\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00153\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00153 to 0.00151, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00151 to 0.00137, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.00137 to 0.00132, saving model to /best_model\\model_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 211: val_loss improved from 0.00132 to 0.00128, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.00128 to 0.00123, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.00123 to 0.00118, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.00118 to 0.00113, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.00113 to 0.00106, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.00106 to 0.00099, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.00099 to 0.00092, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.00092 to 0.00085, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.00085 to 0.00074, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.00074 to 0.00067, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.00067 to 0.00064, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.00064 to 0.00063, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.00063 to 0.00063, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.00063 to 0.00062, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.00062 to 0.00062, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.00062 to 0.00059, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.00059 to 0.00055, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.00055 to 0.00052, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.00052 to 0.00048, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.00048 to 0.00043, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.00043 to 0.00039, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.00039 to 0.00037, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.00037 to 0.00036, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.00036 to 0.00036, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00036\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00036\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00036\n",
      "\n",
      "Epoch 238: val_loss improved from 0.00036 to 0.00035, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 262: val_loss improved from 0.00035 to 0.00035, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00035\n",
      "\n",
      "Epoch 278: val_loss improved from 0.00035 to 0.00035, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.00035 to 0.00033, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.00033 to 0.00032, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.00032 to 0.00029, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.00029 to 0.00027, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.00027 to 0.00025, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.00025 to 0.00025, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00025\n",
      "\n",
      "Epoch 296: val_loss improved from 0.00025 to 0.00021, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.00021 to 0.00018, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.00018 to 0.00017, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00017\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00017\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.6937e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09471, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09471 to 1.09206, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09206 to 1.08837, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08837 to 1.08423, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08423 to 1.07989, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.07989 to 1.07519, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07519 to 1.07011, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07011 to 1.06487, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.06487 to 1.05907, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.05907 to 1.05270, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.05270 to 1.04612, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04612 to 1.03896, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.03896 to 1.03122, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.03122 to 1.02337, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.02337 to 1.01503, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.01503 to 1.00672, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.00672 to 0.99877, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.99877 to 0.99097, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.99097 to 0.98260, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98260 to 0.97297, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.97297 to 0.96280, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.96280 to 0.95169, saving model to /best_model\\model_5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 0.95169 to 0.93951, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.93951 to 0.92537, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.92537 to 0.91097, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.91097 to 0.89429, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.89429 to 0.87522, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.87522 to 0.85405, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.85405 to 0.83069, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.83069 to 0.80490, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.80490 to 0.77715, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.77715 to 0.74710, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.74710 to 0.71498, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.71498 to 0.68726, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.68726 to 0.66199, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.66199 to 0.64484, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.64484 to 0.63214, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.63214 to 0.62502, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.62502 to 0.61416, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.61416 to 0.60715, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.60715 to 0.60239, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.60239 to 0.59050, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.59050 to 0.57569, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.57569 to 0.56345, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.56345 to 0.56270, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.56270 to 0.55777, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.55777 to 0.55363, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.55363\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.55363\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.55363\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.55363\n",
      "\n",
      "Epoch 52: val_loss improved from 0.55363 to 0.55347, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.55347 to 0.53696, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.53696 to 0.52034, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.52034 to 0.51519, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.51519 to 0.50789, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.50789 to 0.49758, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.49758 to 0.49268, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.49268 to 0.48841, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.48841\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.48841\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.48841\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.48841\n",
      "\n",
      "Epoch 64: val_loss improved from 0.48841 to 0.48600, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.48600 to 0.47437, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.47437 to 0.46479, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.46479 to 0.45147, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.45147 to 0.44308, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.44308\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.44308\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.44308\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.44308\n",
      "\n",
      "Epoch 73: val_loss improved from 0.44308 to 0.43034, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.43034 to 0.40503, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.40503 to 0.38230, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.38230 to 0.36866, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.36866 to 0.36221, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.36221 to 0.36170, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.36170\n",
      "\n",
      "Epoch 88: val_loss improved from 0.36170 to 0.35881, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.35881 to 0.33710, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.33710 to 0.33000, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.33000 to 0.32594, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.32594\n",
      "\n",
      "Epoch 98: val_loss improved from 0.32594 to 0.32391, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.32391 to 0.31834, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.31834 to 0.31681, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.31681 to 0.31605, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.31605 to 0.31561, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.31561\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.31561\n",
      "\n",
      "Epoch 105: val_loss improved from 0.31561 to 0.31462, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.31462 to 0.31372, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.31372 to 0.31244, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.31244 to 0.31187, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.31187 to 0.31147, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.31147 to 0.31122, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.31122 to 0.31115, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.31115\n",
      "\n",
      "Epoch 123: val_loss improved from 0.31115 to 0.31110, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.31110 to 0.31084, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.31084 to 0.31049, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.31049 to 0.31039, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.31039 to 0.31034, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.31034 to 0.31031, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.31031\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.31031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: val_loss did not improve from 0.31031\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.31031\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.31031\n",
      "\n",
      "Epoch 134: val_loss improved from 0.31031 to 0.31030, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.31030 to 0.30996, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.30996 to 0.30987, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.30987 to 0.30982, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.30982 to 0.30967, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.30967 to 0.30948, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.30948 to 0.30939, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.30939 to 0.30925, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.30925 to 0.30908, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.30908 to 0.30898, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.30898\n",
      "\n",
      "Epoch 157: val_loss improved from 0.30898 to 0.30872, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.30872 to 0.30845, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.30845 to 0.30824, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.30824 to 0.30807, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.30807\n",
      "\n",
      "Epoch 186: val_loss improved from 0.30807 to 0.30800, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.30800 to 0.30788, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.30788 to 0.30783, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.30783 to 0.30782, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.30782\n",
      "\n",
      "Epoch 219: val_loss improved from 0.30782 to 0.30767, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.30767 to 0.30748, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.30748 to 0.30735, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.30735 to 0.30727, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.30727 to 0.30720, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.30720\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.30720\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.30720\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.30720\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.30720\n",
      "\n",
      "Epoch 229: val_loss improved from 0.30720 to 0.30718, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.30718 to 0.30664, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.30664 to 0.30620, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.30620 to 0.30585, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.30585 to 0.30554, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.30554 to 0.30525, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.30525 to 0.30498, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.30498 to 0.30471, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.30471 to 0.30442, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.30442 to 0.30410, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.30410 to 0.30308, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.30308 to 0.30207, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.30207 to 0.30114, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.30114 to 0.30010, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.30010 to 0.29875, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.29875 to 0.29741, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.29741 to 0.29507, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.29507 to 0.29163, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.29163 to 0.28755, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.28755 to 0.28555, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.28555\n",
      "\n",
      "Epoch 255: val_loss improved from 0.28555 to 0.28325, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.28325 to 0.27828, saving model to /best_model\\model_5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 257: val_loss improved from 0.27828 to 0.27110, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.27110 to 0.26125, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.26125\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.26125\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.26125\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.26125\n",
      "\n",
      "Epoch 263: val_loss improved from 0.26125 to 0.25952, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.25952 to 0.23678, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.23678 to 0.20439, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.20439 to 0.18628, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.18628\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.18628\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.18628\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.18628\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.18628\n",
      "\n",
      "Epoch 272: val_loss improved from 0.18628 to 0.18295, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.18295 to 0.14438, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.14438 to 0.12574, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.12574 to 0.11047, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.11047 to 0.09409, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.09409 to 0.08282, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.08282 to 0.07813, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.07813 to 0.06919, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.06919 to 0.05771, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.05771 to 0.04712, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.04712 to 0.03983, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.03983 to 0.03628, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.03628 to 0.03440, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.03440 to 0.02874, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.02874 to 0.02733, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.02733\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.02733\n",
      "\n",
      "Epoch 289: val_loss improved from 0.02733 to 0.02593, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.02593 to 0.02361, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.02361 to 0.02347, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.02347 to 0.02221, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.02221 to 0.01938, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.01938 to 0.01825, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.01825\n",
      "\n",
      "Epoch 299: val_loss improved from 0.01825 to 0.01653, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.01653 to 0.01399, saving model to /best_model\\model_5.h5\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09383, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09383 to 1.08855, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08855 to 1.08348, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08348 to 1.07855, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.07855 to 1.07314, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.07314 to 1.06725, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.06725 to 1.06081, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.06081 to 1.05432, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.05432 to 1.04810, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.04810 to 1.04131, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.04131 to 1.03440, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.03440 to 1.02757, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.02757 to 1.02002, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.02002 to 1.01144, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.01144 to 1.00218, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.00218 to 0.99298, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.99298 to 0.98415, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.98415 to 0.97482, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.97482 to 0.96434, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.96434 to 0.95209, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.95209 to 0.93794, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.93794 to 0.92266, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.92266 to 0.90606, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.90606 to 0.88649, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.88649 to 0.86376, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.86376 to 0.83979, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.83979 to 0.81398, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.81398 to 0.78480, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.78480 to 0.75319, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.75319 to 0.72003, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.72003 to 0.68608, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.68608 to 0.65301, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.65301 to 0.62184, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.62184 to 0.59449, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.59449 to 0.57033, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.57033 to 0.55192, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.55192 to 0.53785, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.53785 to 0.52882, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.52882 to 0.51838, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.51838 to 0.50640, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.50640 to 0.49937, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.49937 to 0.49345, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.49345 to 0.48888, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.48888 to 0.48426, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.48426 to 0.48031, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.48031 to 0.47712, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.47712 to 0.47417, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.47417 to 0.47131, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.47131 to 0.46857, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.46857 to 0.46588, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss improved from 0.46588 to 0.46295, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.46295 to 0.45969, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.45969 to 0.45646, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.45646 to 0.45305, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.45305 to 0.44974, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.44974 to 0.44647, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.44647 to 0.44362, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.44362 to 0.44054, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.44054 to 0.43690, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.43690 to 0.43304, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.43304 to 0.42941, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.42941 to 0.42587, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.42587 to 0.42254, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.42254 to 0.41901, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.41901 to 0.41520, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.41520 to 0.41128, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.41128 to 0.40728, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.40728 to 0.40332, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.40332 to 0.39920, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.39920 to 0.39510, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.39510 to 0.39127, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.39127 to 0.38729, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.38729 to 0.38247, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.38247 to 0.37695, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.37695 to 0.37296, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.37296 to 0.36981, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.36981 to 0.36618, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.36618 to 0.35966, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.35966 to 0.35338, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.35338 to 0.34861, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.34861 to 0.34369, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.34369 to 0.33735, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.33735 to 0.32757, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.32757 to 0.31496, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.31496 to 0.30378, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.30378 to 0.29703, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.29703 to 0.29055, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.29055 to 0.28467, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.28467 to 0.27032, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.27032 to 0.25517, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.25517 to 0.23052, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.23052 to 0.21972, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.21972 to 0.21030, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.21030 to 0.18527, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.18527 to 0.15747, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.15747 to 0.13850, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.13850 to 0.12344, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.12344 to 0.11626, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.11626 to 0.10157, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.10157 to 0.08851, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.08851 to 0.07961, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.07961 to 0.07148, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.07148 to 0.06038, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.06038 to 0.05879, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.05879 to 0.05808, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.05808 to 0.05072, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.05072 to 0.04698, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.04698 to 0.04567, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.04567 to 0.04417, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.04417 to 0.03922, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.03922 to 0.03357, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.03357\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.03357\n",
      "\n",
      "Epoch 114: val_loss improved from 0.03357 to 0.02884, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.02884 to 0.02575, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.02575 to 0.02534, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.02534 to 0.02327, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.02327 to 0.02099, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.02099 to 0.02015, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 123: val_loss improved from 0.02015 to 0.01789, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.01789 to 0.01413, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.01413 to 0.01229, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.01229 to 0.01159, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.01159 to 0.01063, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.01063\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.01063\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01063\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01063\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01063\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01063 to 0.00933, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.00933 to 0.00863, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.00863 to 0.00859, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00859\n",
      "\n",
      "Epoch 139: val_loss improved from 0.00859 to 0.00800, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.00800 to 0.00739, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.00739\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.00739\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00739\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00739\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00739\n",
      "\n",
      "Epoch 146: val_loss improved from 0.00739 to 0.00647, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.00647 to 0.00550, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 148: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00550\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00550 to 0.00459, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00459 to 0.00403, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00403\n",
      "\n",
      "Epoch 174: val_loss improved from 0.00403 to 0.00359, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00359 to 0.00320, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00320 to 0.00295, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.00295 to 0.00292, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.00292 to 0.00291, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.00291 to 0.00276, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00276 to 0.00247, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.00247 to 0.00202, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00202 to 0.00185, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.00185 to 0.00176, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00176\n",
      "\n",
      "Epoch 240: val_loss improved from 0.00176 to 0.00172, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.00172 to 0.00169, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.00169 to 0.00156, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.00156 to 0.00139, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.00139 to 0.00126, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 253: val_loss improved from 0.00126 to 0.00125, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.00125 to 0.00117, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.00117 to 0.00112, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.00112 to 0.00105, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 267: val_loss improved from 0.00105 to 0.00096, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.00096 to 0.00082, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.00082 to 0.00082, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 291: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00082\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00082\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1991e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09535, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09535 to 1.09237, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09237 to 1.08933, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08933 to 1.08604, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08604 to 1.08259, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08259 to 1.07908, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07908 to 1.07530, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07530 to 1.07103, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07103 to 1.06664, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.06664 to 1.06211, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06211 to 1.05717, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.05717 to 1.05212, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05212 to 1.04686, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.04686 to 1.04156, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.04156 to 1.03529, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.03529 to 1.02900, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.02900 to 1.02204, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.02204 to 1.01478, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.01478 to 1.00697, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.00697 to 0.99912, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.99912 to 0.99012, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.99012 to 0.97993, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.97993 to 0.96865, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.96865 to 0.95694, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.95694 to 0.94460, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.94460 to 0.93057, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.93057 to 0.91480, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.91480 to 0.89767, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.89767 to 0.87953, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.87953 to 0.86049, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.86049 to 0.84053, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.84053 to 0.82073, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.82073 to 0.80120, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.80120 to 0.78058, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.78058 to 0.75993, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.75993 to 0.73883, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.73883 to 0.71703, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.71703 to 0.69398, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.69398 to 0.66981, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.66981 to 0.64611, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.64611 to 0.62310, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.62310 to 0.59993, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.59993 to 0.57693, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.57693 to 0.55333, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.55333 to 0.53255, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.53255 to 0.51761, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.51761 to 0.50934, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.50934 to 0.49251, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.49251 to 0.46835, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.46835 to 0.44770, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.44770 to 0.43626, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.43626\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.43626\n",
      "\n",
      "Epoch 54: val_loss improved from 0.43626 to 0.43260, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.43260 to 0.40206, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.40206 to 0.37733, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.37733 to 0.36488, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.36488 to 0.35675, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.35675 to 0.34464, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.34464 to 0.31921, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.31921 to 0.30488, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.30488 to 0.28866, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.28866 to 0.27498, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.27498 to 0.25683, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.25683 to 0.24591, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.24591 to 0.24388, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.24388 to 0.22624, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.22624 to 0.21055, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.21055 to 0.20578, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.20578 to 0.19289, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.19289 to 0.16981, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.16981 to 0.15806, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.15806 to 0.14757, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.14757\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.14757\n",
      "\n",
      "Epoch 76: val_loss improved from 0.14757 to 0.13902, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.13902 to 0.12648, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.12648 to 0.11854, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.11854\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.11854\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.11854\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.11854\n",
      "\n",
      "Epoch 83: val_loss improved from 0.11854 to 0.11028, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.11028\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.11028\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.11028\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.11028\n",
      "\n",
      "Epoch 88: val_loss improved from 0.11028 to 0.10500, saving model to /best_model\\model_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89: val_loss improved from 0.10500 to 0.05859, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05859\n",
      "\n",
      "Epoch 102: val_loss improved from 0.05859 to 0.05083, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.05083\n",
      "\n",
      "Epoch 154: val_loss improved from 0.05083 to 0.04599, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.04599 to 0.04218, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.04218\n",
      "\n",
      "Epoch 183: val_loss improved from 0.04218 to 0.04061, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.04061\n",
      "\n",
      "Epoch 195: val_loss improved from 0.04061 to 0.03497, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.03497 to 0.02100, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.02100 to 0.01427, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.01427 to 0.01360, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.01360\n",
      "\n",
      "Epoch 230: val_loss improved from 0.01360 to 0.01333, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.01333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 245: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.01333\n",
      "\n",
      "Epoch 251: val_loss improved from 0.01333 to 0.00827, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00827\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00827\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09582, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09582 to 1.09273, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09273 to 1.08971, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08971 to 1.08644, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08644 to 1.08307, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08307 to 1.07973, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07973 to 1.07661, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07661 to 1.07311, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07311 to 1.06965, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.06965 to 1.06597, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06597 to 1.06177, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06177 to 1.05728, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05728 to 1.05329, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.05329 to 1.04861, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.04861 to 1.04371, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.04371 to 1.03806, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.03806 to 1.03195, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.03195 to 1.02515, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.02515 to 1.01771, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.01771 to 1.00957, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.00957 to 1.00111, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.00111 to 0.99223, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.99223 to 0.98280, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.98280 to 0.97304, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.97304 to 0.96318, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.96318 to 0.95284, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.95284 to 0.94243, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.94243 to 0.93108, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.93108 to 0.91977, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.91977 to 0.90924, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.90924 to 0.90158, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.90158 to 0.89321, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.89321 to 0.88488, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.88488 to 0.87680, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.87680 to 0.86758, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.86758 to 0.85894, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.85894 to 0.85326, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.85326 to 0.85029, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.85029 to 0.84482, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.84482 to 0.83761, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.83761\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.83761\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.83761\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.83761\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.83761\n",
      "\n",
      "Epoch 46: val_loss improved from 0.83761 to 0.82485, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.82485 to 0.82371, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.82371\n",
      "\n",
      "Epoch 60: val_loss improved from 0.82371 to 0.80140, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.80140 to 0.78507, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.78507 to 0.77277, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.77277\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.77277\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.77277\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.77277\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.77277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: val_loss improved from 0.77277 to 0.76776, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.76776\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.76776\n",
      "\n",
      "Epoch 71: val_loss improved from 0.76776 to 0.73404, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.73404 to 0.70075, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.70075 to 0.69683, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.69683 to 0.68074, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.68074 to 0.67000, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.67000 to 0.62046, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.62046 to 0.59862, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.59862\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.59862\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.59862\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.59862\n",
      "\n",
      "Epoch 82: val_loss improved from 0.59862 to 0.59287, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.59287\n",
      "\n",
      "Epoch 89: val_loss improved from 0.59287 to 0.58462, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.58462 to 0.56028, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.56028\n",
      "\n",
      "Epoch 98: val_loss improved from 0.56028 to 0.55144, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.55144\n",
      "\n",
      "Epoch 125: val_loss improved from 0.55144 to 0.52670, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.52670 to 0.51758, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.51758\n",
      "\n",
      "Epoch 149: val_loss improved from 0.51758 to 0.49780, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.49780 to 0.47925, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.47925\n",
      "\n",
      "Epoch 173: val_loss improved from 0.47925 to 0.47860, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.47860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 218: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.47860\n",
      "\n",
      "Epoch 236: val_loss improved from 0.47860 to 0.47620, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.47620 to 0.47268, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.47268\n",
      "\n",
      "Epoch 298: val_loss improved from 0.47268 to 0.36270, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.36270\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.36270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3627 - accuracy: 0.8889\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09256, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09256 to 1.08686, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08686 to 1.08013, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08013 to 1.07351, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.07351 to 1.06663, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.06663 to 1.06018, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.06018 to 1.05348, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.05348 to 1.04647, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.04647 to 1.03945, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.03945 to 1.03323, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.03323 to 1.02617, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.02617 to 1.01917, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.01917 to 1.01278, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.01278 to 1.00543, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.00543 to 0.99786, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.99786 to 0.99053, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.99053 to 0.98317, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.98317 to 0.97390, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.97390 to 0.96346, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.96346 to 0.95214, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.95214 to 0.94007, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.94007 to 0.92715, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.92715 to 0.91196, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.91196 to 0.89454, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.89454 to 0.87731, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.87731 to 0.85988, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.85988 to 0.84355, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.84355 to 0.82820, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.82820 to 0.81265, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.81265 to 0.79926, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.79926 to 0.78566, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.78566 to 0.77266, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.77266 to 0.76175, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.76175 to 0.75082, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.75082 to 0.73686, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.73686 to 0.72493, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.72493 to 0.71246, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.71246 to 0.70004, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.70004 to 0.68759, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.68759 to 0.66849, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.66849 to 0.64342, saving model to /best_model\\model_9.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: val_loss improved from 0.64342 to 0.62339, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.62339 to 0.60531, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.60531 to 0.58737, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.58737 to 0.56891, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.56891 to 0.54728, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.54728 to 0.52275, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.52275 to 0.50421, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.50421 to 0.47795, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.47795 to 0.44179, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.44179 to 0.41697, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.41697 to 0.39583, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.39583 to 0.37890, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.37890 to 0.35325, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.35325 to 0.33412, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.33412 to 0.31651, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.31651 to 0.30149, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.30149 to 0.28610, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.28610 to 0.27302, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.27302 to 0.26078, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.26078 to 0.25098, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.25098 to 0.24748, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.24748 to 0.24450, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.24450 to 0.23514, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.23514 to 0.21996, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.21996 to 0.20835, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.20835 to 0.20017, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.20017\n",
      "\n",
      "Epoch 69: val_loss improved from 0.20017 to 0.19523, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.19523 to 0.17261, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.17261 to 0.16316, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.16316 to 0.16138, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.16138 to 0.15656, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.15656 to 0.14881, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.14881 to 0.13330, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.13330\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.13330\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.13330\n",
      "\n",
      "Epoch 79: val_loss improved from 0.13330 to 0.12784, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.12784 to 0.10883, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.10883\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.10883\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.10883\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.10883\n",
      "\n",
      "Epoch 85: val_loss improved from 0.10883 to 0.10006, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.10006 to 0.09526, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.09526 to 0.09099, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.09099 to 0.08831, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.08831 to 0.08543, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.08543 to 0.08519, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.08519\n",
      "\n",
      "Epoch 97: val_loss improved from 0.08519 to 0.07732, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.07732 to 0.06008, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.06008 to 0.05652, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05652\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05652\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05652\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05652\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05652\n",
      "\n",
      "Epoch 105: val_loss improved from 0.05652 to 0.05547, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.05547 to 0.05257, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.05257 to 0.05060, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05060\n",
      "\n",
      "Epoch 114: val_loss improved from 0.05060 to 0.04507, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.04507\n",
      "\n",
      "Epoch 121: val_loss improved from 0.04507 to 0.04342, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.04342 to 0.04239, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.04239\n",
      "\n",
      "Epoch 136: val_loss improved from 0.04239 to 0.03662, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.03662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 163: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 174: val_loss improved from 0.03662 to 0.03653, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.03653 to 0.03221, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.03221 to 0.02591, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.02591\n",
      "\n",
      "Epoch 178: val_loss improved from 0.02591 to 0.02511, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.02511 to 0.01976, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.01976\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.01976\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.01976\n",
      "\n",
      "Epoch 183: val_loss improved from 0.01976 to 0.01968, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.01968 to 0.01596, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.01596 to 0.01534, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.01534 to 0.01350, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.01350\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.01350\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.01350\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.01350\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.01350\n",
      "\n",
      "Epoch 192: val_loss improved from 0.01350 to 0.01166, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 231: val_loss improved from 0.01166 to 0.00738, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.00738 to 0.00706, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.00706 to 0.00660, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 268: val_loss improved from 0.00660 to 0.00573, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.00573 to 0.00406, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.00406 to 0.00402, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 282: val_loss improved from 0.00402 to 0.00360, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 284: val_loss improved from 0.00360 to 0.00307, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.00307 to 0.00263, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.00263 to 0.00244, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00244\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00244\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00244\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00244\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00244\n",
      "\n",
      "Epoch 292: val_loss improved from 0.00244 to 0.00236, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.00236 to 0.00199, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.00199 to 0.00188, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.00188 to 0.00183, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.00183 to 0.00181, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.00181 to 0.00179, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00179\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09692, saving model to /best_model\\model_10.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.09692 to 1.09482, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09482 to 1.09261, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09261 to 1.08985, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08985 to 1.08723, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08723 to 1.08479, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08479 to 1.08206, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08206 to 1.07908, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07908 to 1.07607, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07607 to 1.07310, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07310 to 1.07003, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.07003 to 1.06662, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06662 to 1.06313, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06313 to 1.05966, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05966 to 1.05613, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05613 to 1.05226, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05226 to 1.04816, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.04816 to 1.04383, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04383 to 1.03909, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.03909 to 1.03381, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.03381 to 1.02816, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.02816 to 1.02226, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.02226 to 1.01516, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.01516 to 1.00722, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.00722 to 0.99903, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99903 to 0.98916, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98916 to 0.97871, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.97871 to 0.96662, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.96662 to 0.95331, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.95331 to 0.93922, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.93922 to 0.92255, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.92255 to 0.90236, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.90236 to 0.88073, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.88073 to 0.85791, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.85791 to 0.83103, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.83103 to 0.80279, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.80279 to 0.77058, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.77058 to 0.73647, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.73647 to 0.70125, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.70125 to 0.66576, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.66576 to 0.62613, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.62613 to 0.58961, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.58961 to 0.55405, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.55405 to 0.52220, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.52220 to 0.49553, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.49553 to 0.47153, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.47153 to 0.44044, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.44044 to 0.41325, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.41325 to 0.39191, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.39191 to 0.39065, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.39065 to 0.36800, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.36800 to 0.33679, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.33679 to 0.31663, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.31663 to 0.30219, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.30219 to 0.29731, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.29731 to 0.28364, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.28364 to 0.26882, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.26882 to 0.25641, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.25641 to 0.24168, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.24168 to 0.22821, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.22821 to 0.22615, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.22615\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.22615\n",
      "\n",
      "Epoch 64: val_loss improved from 0.22615 to 0.21365, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.21365 to 0.20529, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.20529 to 0.20497, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.20497\n",
      "\n",
      "Epoch 75: val_loss improved from 0.20497 to 0.20337, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.20337\n",
      "\n",
      "Epoch 82: val_loss improved from 0.20337 to 0.17742, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.17742 to 0.15483, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.15483 to 0.15111, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.15111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 108: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.15111\n",
      "\n",
      "Epoch 179: val_loss improved from 0.15111 to 0.14143, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.14143 to 0.11976, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.11976 to 0.10684, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.10684 to 0.10448, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.10448\n",
      "\n",
      "Epoch 213: val_loss improved from 0.10448 to 0.07942, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.07942 to 0.07496, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.07496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 267: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.07496\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.07496\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0750 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Wall time: 10min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 앞에 keras 관련 import문이랑 사용자정의함수 돌리고 실행하기\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix # 혼동행렬\n",
    "\n",
    "k = 10\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "acc_score = [] # 정확도를 정리할 리스트\n",
    "cf_matrix = [] # 혼동행렬을 정리할 리스트\n",
    "\n",
    "\n",
    "fold_val = 1\n",
    "for train_index, test_index in kfold.split(x, y.argmax(axis=1)) : \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # create new model\n",
    "    model = model_fn(1) # feature_num\n",
    "    # compile new model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=150) \n",
    "    mc = ModelCheckpoint(get_model_name(fold_val), monitor='val_loss', mode='min',  \n",
    "                         save_best_only=True, verbose = 1)\n",
    "    history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=300, verbose=0,\n",
    "                       callbacks=[es, mc], validation_data=(x_test, y_test))\n",
    "    \n",
    "    model.load_weights(\"/best_model/model_\"+str(fold_val)+\".h5\") \n",
    "    \n",
    "    accuracy = model.evaluate(x_test, y_test)[1]\n",
    "    acc_score.append(accuracy)\n",
    "    \n",
    "    # 혼동행렬\n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    cf_matrix.append(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12ed894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "정확도 평균 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a4ca7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5, 0, 0],\n",
       "        [0, 4, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[5, 0, 0],\n",
       "        [0, 4, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [0, 5, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [0, 5, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[ 4,  0,  0],\n",
       "        [ 0,  4,  0],\n",
       "        [ 0,  0, 10]], dtype=int64)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06383ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86e249c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [1.0, 1.0, 0.8888888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "정확도 평균 : 0.9888888895511627\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63a99e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [1, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 3, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e3c00",
   "metadata": {},
   "source": [
    "# 모델 B (persistence landscape argmax만 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e652d8",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "097eacb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n",
      "(90, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pl_h1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pl_h1\n",
       "0     62\n",
       "1     66\n",
       "2     65\n",
       "3     69\n",
       "4     72"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time1 = pl_time1[[\"pl_h1\"]]\n",
    "df_time2 = pl_time2[[\"pl_h1\"]]\n",
    "df_time3 = pl_time3[[\"pl_h1\"]]\n",
    "df_time4 = pl_time4[[\"pl_h1\"]]\n",
    "df_time5 = pl_time5[[\"pl_h1\"]]\n",
    "df_time6 = pl_time6[[\"pl_h1\"]]\n",
    "\n",
    "print(df_time1.shape)\n",
    "print(df_time2.shape)\n",
    "print(df_time3.shape)\n",
    "print(df_time4.shape)\n",
    "print(df_time5.shape)\n",
    "print(df_time6.shape)\n",
    "df_time1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4350142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02e6d209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_time1\n",
      "pl_h1    61\n",
      "dtype: int64\n",
      "pl_h1    79\n",
      "dtype: int64\n",
      "df_time2\n",
      "pl_h1    65\n",
      "dtype: int64\n",
      "pl_h1    78\n",
      "dtype: int64\n",
      "df_time3\n",
      "pl_h1    62\n",
      "dtype: int64\n",
      "pl_h1    79\n",
      "dtype: int64\n",
      "df_time4\n",
      "pl_h1    59\n",
      "dtype: int64\n",
      "pl_h1    80\n",
      "dtype: int64\n",
      "df_time5\n",
      "pl_h1    0\n",
      "dtype: int64\n",
      "pl_h1    80\n",
      "dtype: int64\n",
      "df_time6\n",
      "pl_h1    0\n",
      "dtype: int64\n",
      "pl_h1    79\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for embedding\n",
    "print(\"df_time1\")\n",
    "print(df_time1.min())\n",
    "print(df_time1.max())\n",
    "print(\"df_time2\")\n",
    "print(df_time2.min())\n",
    "print(df_time2.max())\n",
    "print(\"df_time3\")\n",
    "print(df_time3.min())\n",
    "print(df_time3.max())\n",
    "print(\"df_time4\")\n",
    "print(df_time4.min())\n",
    "print(df_time4.max())\n",
    "print(\"df_time5\")\n",
    "print(df_time5.min())\n",
    "print(df_time5.max())\n",
    "print(\"df_time6\")\n",
    "print(df_time6.min())\n",
    "print(df_time6.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e764e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e2e80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_time1, df_time2, df_time3, df_time4, df_time5, df_time6]\n",
    "x = [] \n",
    "\n",
    "for i in range(90) :\n",
    "    sample_x = [] \n",
    "    for j in range(6) :\n",
    "        sample_x.append(dfs[j].iloc[i])    \n",
    "    x.append(sample_x)\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12156562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 0 : lt, 1 : rt, 2 : hc\n",
    "y = np.array(np.repeat([0,1,2], [22, 22, 46], axis=0)) \n",
    "y = y.reshape(90,-1)\n",
    "\n",
    "# 정답 데이터 one-hot vector 처리\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8fc82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 6, 1)\n",
      "(90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# (90, 6, feature 수)\n",
    "# (90, group 수(=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847e54f",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "329c9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e59734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Dropout, Flatten, Embedding, TimeDistributed, Reshape, Concatenate\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc9b035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embednet(shape): # shape=3\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=81, output_dim=2, input_length=shape))\n",
    "    model.add(Reshape(target_shape=(-1,)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03624c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k겹 때마다 생성할 모델 \n",
    "def model_fn(feature_num) :\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(TimeDistributed(build_embednet(feature_num), input_shape=(6, feature_num))) # embedding용으로 추가\n",
    "\n",
    "    # GRU (Sequence) \n",
    "    model.add(GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu', return_sequences=True))\n",
    "    model.add(GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu')) # , return_sequences=True\n",
    "    \n",
    "    # Classification\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(.15))\n",
    "    model.add(Dense(3, activation='softmax')) # final classification\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8369958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 6, 2)             162       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 6, 8)              288       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 8)                 432       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                144       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,349\n",
      "Trainable params: 1,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn(1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfeb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6e86ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k겹 때마다 가장 좋은 모델을 저장하기 위한 파일 이름\n",
    "def get_model_name(k):\n",
    "    return '/best_model/model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fd96a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09708, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09708 to 1.09607, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09607 to 1.09492, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09492 to 1.09381, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09381 to 1.09261, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.09261 to 1.09120, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.09120 to 1.08976, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08976 to 1.08818, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08818 to 1.08631, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08631 to 1.08429, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.08429 to 1.08201, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08201 to 1.07928, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.07928 to 1.07681, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.07681 to 1.07360, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.07360 to 1.07032, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.07032 to 1.06675, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.06675 to 1.06272, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.06272 to 1.05774, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.05774 to 1.05229, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.05229 to 1.04662, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.04662 to 1.03896, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.03896 to 1.03005, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.03005 to 1.02080, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.02080 to 1.00998, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.00998 to 0.99856, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99856 to 0.98564, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98564 to 0.97198, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.97198 to 0.95706, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.95706 to 0.94445, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.94445 to 0.93037, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.93037 to 0.91278, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.91278 to 0.89282, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.89282 to 0.87033, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.87033 to 0.84737, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.84737 to 0.82591, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.82591 to 0.80695, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.80695 to 0.78851, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.78851 to 0.76704, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.76704 to 0.75110, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.75110 to 0.74008, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.74008 to 0.73007, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.73007 to 0.71655, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.71655 to 0.70292, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.70292 to 0.68951, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.68951 to 0.67882, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.67882 to 0.66955, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.66955 to 0.65805, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.65805 to 0.64692, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.64692 to 0.63151, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.63151 to 0.61833, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.61833 to 0.61405, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.61405 to 0.61091, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.61091 to 0.60812, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.60812 to 0.60343, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.60343 to 0.59630, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.59630 to 0.58985, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.58985 to 0.58840, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.58840 to 0.58561, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.58561 to 0.58395, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.58395 to 0.58222, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.58222 to 0.57679, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.57679 to 0.56992, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.56992 to 0.56647, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.56647\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.56647\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.56647\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.56647\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.56647\n",
      "\n",
      "Epoch 69: val_loss improved from 0.56647 to 0.56479, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.56479 to 0.56009, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.56009 to 0.55848, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.55848 to 0.55524, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.55524 to 0.55465, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.55465 to 0.55291, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.55291\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.55291\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.55291\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.55291\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.55291\n",
      "\n",
      "Epoch 80: val_loss improved from 0.55291 to 0.52737, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.52737 to 0.50956, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.50956 to 0.50575, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.50575\n",
      "\n",
      "Epoch 89: val_loss improved from 0.50575 to 0.50220, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.50220 to 0.48701, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.48701 to 0.48465, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.48465 to 0.48446, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.48446 to 0.48041, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.48041 to 0.47699, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.47699 to 0.46776, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.46776 to 0.45915, saving model to /best_model\\model_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97: val_loss did not improve from 0.45915\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.45915\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.45915\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.45915\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.45915\n",
      "\n",
      "Epoch 102: val_loss improved from 0.45915 to 0.45903, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.45903 to 0.44988, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.44988 to 0.44405, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.44405 to 0.44092, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.44092 to 0.43973, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.43973\n",
      "\n",
      "Epoch 108: val_loss improved from 0.43973 to 0.43910, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.43910 to 0.43874, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.43874\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.43874\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.43874\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.43874\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.43874\n",
      "\n",
      "Epoch 115: val_loss improved from 0.43874 to 0.43816, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.43816 to 0.43794, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.43794\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.43794\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.43794\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.43794\n",
      "\n",
      "Epoch 121: val_loss improved from 0.43794 to 0.43692, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.43692 to 0.43547, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.43547 to 0.43386, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.43386 to 0.43219, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.43219 to 0.43196, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 130: val_loss improved from 0.43196 to 0.42672, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.42672 to 0.41998, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.41998 to 0.41894, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.41894\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.41894\n",
      "\n",
      "Epoch 135: val_loss improved from 0.41894 to 0.41860, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.41860\n",
      "\n",
      "Epoch 137: val_loss improved from 0.41860 to 0.41745, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.41745 to 0.41303, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.41303 to 0.41020, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.41020\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.41020\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.41020\n",
      "\n",
      "Epoch 143: val_loss improved from 0.41020 to 0.40736, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.40736\n",
      "\n",
      "Epoch 150: val_loss improved from 0.40736 to 0.40501, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.40501 to 0.40392, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.40392\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.40392\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.40392\n",
      "\n",
      "Epoch 155: val_loss improved from 0.40392 to 0.40253, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.40253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 239: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.40253\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.40253\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4025 - accuracy: 0.8889\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09705, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09705 to 1.09551, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09551 to 1.09383, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09383 to 1.09227, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09227 to 1.09068, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.09068 to 1.08901, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08901 to 1.08760, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08760 to 1.08609, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08609 to 1.08441, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08441 to 1.08285, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.08285 to 1.08149, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08149 to 1.08028, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.08028 to 1.07904, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.07904 to 1.07782, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.07782 to 1.07653, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.07653 to 1.07517, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.07517 to 1.07337, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.07337 to 1.07219, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.07219 to 1.07163, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.07163 to 1.07082, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.07082 to 1.06969, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.06969 to 1.06798, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.06798 to 1.06630, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.06630 to 1.06332, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.06332 to 1.05903, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.05903 to 1.05133, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.05133 to 1.04459, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 1.04459 to 1.04076, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 1.04076 to 1.04001, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.04001\n",
      "\n",
      "Epoch 51: val_loss improved from 1.04001 to 1.03774, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 1.03774 to 1.02809, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 1.02809 to 1.01822, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 1.01822 to 1.00802, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 1.00802 to 0.99989, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.99989 to 0.99322, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.99322 to 0.98949, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.98949\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.98949\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.98949\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.98949\n",
      "\n",
      "Epoch 62: val_loss improved from 0.98949 to 0.98669, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.98669 to 0.98002, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.98002 to 0.96855, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.96855 to 0.96030, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.96030 to 0.95580, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss improved from 0.95580 to 0.95307, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.95307 to 0.94364, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.94364 to 0.93867, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.93867\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.93867\n",
      "Epoch 219: early stopping\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.9387 - accuracy: 0.5556\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09547, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09547 to 1.09296, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09296 to 1.09024, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09024 to 1.08751, saving model to /best_model\\model_3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 1.08751 to 1.08445, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08445 to 1.08127, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08127 to 1.07760, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07760 to 1.07354, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07354 to 1.06908, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.06908 to 1.06424, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06424 to 1.05936, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.05936 to 1.05422, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05422 to 1.04839, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.04839 to 1.04175, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.04175 to 1.03428, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.03428 to 1.02582, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.02582 to 1.01653, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.01653 to 1.00664, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.00664 to 0.99599, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.99599 to 0.98324, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.98324 to 0.96933, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.96933 to 0.95473, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.95473 to 0.93878, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.93878 to 0.92222, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.92222 to 0.90509, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.90509 to 0.88790, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.88790 to 0.87035, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.87035 to 0.85264, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.85264 to 0.83537, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.83537 to 0.82115, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.82115 to 0.80754, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.80754 to 0.79535, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.79535 to 0.78527, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.78527 to 0.77690, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.77690 to 0.77078, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.77078 to 0.76679, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.76679 to 0.76487, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.76487 to 0.76443, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.76443\n",
      "\n",
      "Epoch 40: val_loss improved from 0.76443 to 0.76417, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.76417\n",
      "\n",
      "Epoch 42: val_loss improved from 0.76417 to 0.76359, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.76359 to 0.76330, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.76330 to 0.76228, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.76228 to 0.76215, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.76215 to 0.76204, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.76204 to 0.75991, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.75991 to 0.75876, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.75876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 134: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.75876\n",
      "\n",
      "Epoch 160: val_loss improved from 0.75876 to 0.75269, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.75269 to 0.74943, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.74943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 297: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.74943\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.74943\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7494 - accuracy: 0.7778\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09765, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09765 to 1.09624, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09624 to 1.09488, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09488 to 1.09371, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09371 to 1.09248, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.09248 to 1.09124, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.09124 to 1.08997, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08997 to 1.08867, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08867 to 1.08731, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08731 to 1.08596, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.08596 to 1.08418, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08418 to 1.08293, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.08293 to 1.08110, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.08110 to 1.07891, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.07891 to 1.07694, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.07694 to 1.07501, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.07501 to 1.07284, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.07284 to 1.07040, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.07040 to 1.06788, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.06788 to 1.06515, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.06515 to 1.06223, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.06223 to 1.05940, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.05940 to 1.05564, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.05564 to 1.05161, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.05161 to 1.04741, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.04741 to 1.04291, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.04291 to 1.03807, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 1.03807 to 1.03298, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 1.03298 to 1.02743, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 1.02743 to 1.02013, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 1.02013 to 1.01143, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 1.01143 to 1.00121, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 1.00121 to 0.98848, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.98848 to 0.97467, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.97467 to 0.96002, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.96002 to 0.94445, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.94445 to 0.92686, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.92686 to 0.90980, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.90980 to 0.89484, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.89484 to 0.88047, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.88047 to 0.86811, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.86811 to 0.85746, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.85746 to 0.84802, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.84802 to 0.83726, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.83726 to 0.82350, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.82350 to 0.80655, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.80655 to 0.78659, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.78659 to 0.76541, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.76541 to 0.74962, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.74962 to 0.73675, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.73675 to 0.72679, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.72679 to 0.71860, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.71860 to 0.71064, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.71064 to 0.70402, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.70402 to 0.69921, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.69921 to 0.69461, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.69461 to 0.68855, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.68855 to 0.68141, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.68141 to 0.67625, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.67625 to 0.67415, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.67415\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.67415\n",
      "\n",
      "Epoch 63: val_loss improved from 0.67415 to 0.67298, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.67298 to 0.66801, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.66801 to 0.66218, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.66218 to 0.65606, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.65606 to 0.65140, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.65140 to 0.64861, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.64861 to 0.64772, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.64772\n",
      "\n",
      "Epoch 84: val_loss improved from 0.64772 to 0.64082, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.64082 to 0.64023, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.64023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.64023\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.64023\n",
      "Epoch 235: early stopping\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6402 - accuracy: 0.6667\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09723, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09723 to 1.09539, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09539 to 1.09347, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09347 to 1.09072, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09072 to 1.08774, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08774 to 1.08458, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08458 to 1.08110, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08110 to 1.07704, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07704 to 1.07316, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07316 to 1.06835, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06835 to 1.06343, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06343 to 1.05769, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05769 to 1.05209, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.05209 to 1.04703, saving model to /best_model\\model_5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 1.04703 to 1.04122, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.04122 to 1.03559, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.03559 to 1.03032, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.03032 to 1.02493, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.02493 to 1.01906, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.01906 to 1.01367, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.01367 to 1.00838, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.00838 to 1.00364, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.00364 to 0.99931, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.99931 to 0.99501, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.99501 to 0.99064, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99064 to 0.98622, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98622 to 0.98160, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.98160 to 0.97683, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.97683 to 0.97266, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.97266 to 0.96909, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.96909 to 0.96429, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.96429 to 0.95858, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.95858 to 0.95194, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.95194 to 0.94463, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.94463 to 0.93734, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.93734 to 0.93012, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.93012 to 0.92458, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.92458 to 0.91785, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.91785 to 0.91217, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.91217 to 0.90617, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.90617 to 0.90031, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.90031 to 0.89471, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.89471 to 0.89012, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.89012 to 0.88702, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.88702 to 0.88412, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.88412 to 0.88239, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.88239 to 0.88095, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.88095 to 0.87897, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.87897 to 0.87634, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.87634 to 0.87342, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.87342 to 0.86866, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.86866 to 0.86660, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.86660\n",
      "\n",
      "Epoch 54: val_loss improved from 0.86660 to 0.86366, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.86366 to 0.85877, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.85877 to 0.85474, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.85474 to 0.85265, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.85265 to 0.85058, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.85058 to 0.84845, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.84845 to 0.84767, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.84767\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.84767\n",
      "\n",
      "Epoch 63: val_loss improved from 0.84767 to 0.84520, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.84520 to 0.84191, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.84191 to 0.83867, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.83867 to 0.83145, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.83145 to 0.82251, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.82251 to 0.81419, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.81419 to 0.80861, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.80861 to 0.80435, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.80435 to 0.80359, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.80359\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.80359\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.80359\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.80359\n",
      "\n",
      "Epoch 76: val_loss improved from 0.80359 to 0.80034, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.80034 to 0.79643, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.79643 to 0.79317, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.79317 to 0.79199, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.79199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 130: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.79199\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.79199\n",
      "Epoch 229: early stopping\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7920 - accuracy: 0.6667\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09685, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09685 to 1.09484, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09484 to 1.09284, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09284 to 1.09092, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09092 to 1.08883, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08883 to 1.08661, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08661 to 1.08418, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08418 to 1.08182, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08182 to 1.07942, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07942 to 1.07683, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07683 to 1.07347, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.07347 to 1.06985, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06985 to 1.06588, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06588 to 1.06166, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.06166 to 1.05750, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05750 to 1.05307, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05307 to 1.04941, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.04941 to 1.04372, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04372 to 1.03716, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.03716 to 1.03111, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.03111 to 1.02654, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.02654 to 1.02004, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.02004 to 1.01562, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.01562 to 1.00657, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.00657 to 0.99551, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99551 to 0.98671, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98671 to 0.97736, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.97736 to 0.96843, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.96843 to 0.95359, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.95359 to 0.93902, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.93902 to 0.92589, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.92589 to 0.91372, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.91372 to 0.90008, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.90008 to 0.88589, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.88589 to 0.87155, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.87155 to 0.86065, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.86065 to 0.85203, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.85203 to 0.84045, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.84045 to 0.82362, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.82362 to 0.81184, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.81184 to 0.79739, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.79739 to 0.78588, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.78588 to 0.77524, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.77524 to 0.77509, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.77509 to 0.76467, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.76467 to 0.75305, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.75305 to 0.73732, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.73732 to 0.72730, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.72730 to 0.72294, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.72294 to 0.72088, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.72088\n",
      "\n",
      "Epoch 52: val_loss improved from 0.72088 to 0.71805, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.71805 to 0.71189, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.71189 to 0.70579, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.70579\n",
      "\n",
      "Epoch 56: val_loss improved from 0.70579 to 0.70041, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.70041 to 0.69296, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.69296 to 0.68395, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.68395 to 0.67960, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.67960 to 0.66971, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.66971 to 0.65578, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.65578 to 0.64706, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.64706 to 0.64279, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.64279\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.64279\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.64279\n",
      "\n",
      "Epoch 67: val_loss improved from 0.64279 to 0.64121, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.64121 to 0.63864, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.63864 to 0.63278, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.63278\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.63278\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.63278\n",
      "\n",
      "Epoch 73: val_loss improved from 0.63278 to 0.62352, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.62352 to 0.61948, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.61948 to 0.61580, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.61580 to 0.61553, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.61553\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.61553\n",
      "\n",
      "Epoch 79: val_loss improved from 0.61553 to 0.61347, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.61347 to 0.60838, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.60838\n",
      "\n",
      "Epoch 82: val_loss improved from 0.60838 to 0.60441, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.60441\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.60441\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.60441\n",
      "\n",
      "Epoch 86: val_loss improved from 0.60441 to 0.59664, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.59664 to 0.57963, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.57963 to 0.57905, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.57905 to 0.57408, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.57408 to 0.57291, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.57291\n",
      "\n",
      "Epoch 98: val_loss improved from 0.57291 to 0.56763, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.56763 to 0.55549, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.55549\n",
      "\n",
      "Epoch 101: val_loss improved from 0.55549 to 0.55527, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.55527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.55527\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.55527\n",
      "Epoch 251: early stopping\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5553 - accuracy: 0.7778\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09809, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09809 to 1.09556, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09556 to 1.09299, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09299 to 1.09024, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09024 to 1.08747, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08747 to 1.08431, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08431 to 1.08074, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08074 to 1.07723, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07723 to 1.07363, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07363 to 1.07015, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07015 to 1.06588, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06588 to 1.06163, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06163 to 1.05739, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.05739 to 1.05300, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05300 to 1.04789, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.04789 to 1.04184, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.04184 to 1.03581, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.03581 to 1.02895, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.02895 to 1.02232, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.02232 to 1.01562, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.01562 to 1.00955, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.00955 to 1.00179, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.00179 to 0.99100, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.99100 to 0.97956, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.97956 to 0.96941, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.96941 to 0.95981, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.95981 to 0.95191, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.95191 to 0.94320, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.94320 to 0.92934, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.92934 to 0.91271, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.91271 to 0.89494, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.89494 to 0.87842, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.87842 to 0.86239, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.86239 to 0.84939, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.84939 to 0.83525, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.83525 to 0.81641, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.81641 to 0.79203, saving model to /best_model\\model_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss improved from 0.79203 to 0.76882, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.76882 to 0.74761, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.74761 to 0.72944, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.72944 to 0.71334, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.71334 to 0.69660, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.69660 to 0.68062, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.68062 to 0.66840, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.66840 to 0.65775, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.65775 to 0.64722, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.64722 to 0.62715, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.62715 to 0.61069, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.61069 to 0.59740, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.59740 to 0.58854, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.58854 to 0.57737, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.57737 to 0.56414, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.56414 to 0.55773, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.55773 to 0.55187, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.55187 to 0.54961, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.54961\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.54961\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.54961\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.54961\n",
      "\n",
      "Epoch 60: val_loss improved from 0.54961 to 0.54763, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.54763\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.54763\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.54763\n",
      "\n",
      "Epoch 64: val_loss improved from 0.54763 to 0.53937, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.53937 to 0.51131, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.51131 to 0.49639, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.49639 to 0.48980, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.48980 to 0.48919, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.48919\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.48919\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.48919\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.48919\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.48919\n",
      "\n",
      "Epoch 74: val_loss improved from 0.48919 to 0.47002, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.47002 to 0.46290, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.46290 to 0.45708, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.45708 to 0.45547, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.45547 to 0.45317, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.45317 to 0.45018, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.45018 to 0.44810, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.44810\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.44810\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.44810\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.44810\n",
      "\n",
      "Epoch 85: val_loss improved from 0.44810 to 0.44662, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.44662 to 0.44177, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.44177 to 0.43673, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.43673 to 0.43465, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.43465 to 0.43316, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.43316 to 0.43196, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.43196\n",
      "\n",
      "Epoch 96: val_loss improved from 0.43196 to 0.42666, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.42666 to 0.42296, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.42296 to 0.42027, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.42027 to 0.41744, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.41744 to 0.41483, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.41483 to 0.41299, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.41299\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.41299\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.41299\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.41299\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.41299\n",
      "\n",
      "Epoch 107: val_loss improved from 0.41299 to 0.41230, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.41230\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.41230\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.41230\n",
      "\n",
      "Epoch 111: val_loss improved from 0.41230 to 0.41028, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.41028 to 0.40371, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.40371 to 0.39945, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.39945 to 0.39621, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.39621 to 0.39555, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.39555\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.39555\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.39555\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.39555\n",
      "\n",
      "Epoch 120: val_loss improved from 0.39555 to 0.38921, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.38921 to 0.38234, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.38234\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.38234\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.38234\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.38234\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.38234\n",
      "\n",
      "Epoch 127: val_loss improved from 0.38234 to 0.37560, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.37560 to 0.37243, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.37243 to 0.37130, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.37130\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.37130\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.37130\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.37130\n",
      "\n",
      "Epoch 134: val_loss improved from 0.37130 to 0.36884, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.36884 to 0.36730, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.36730 to 0.36544, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.36544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 153: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.36544\n",
      "\n",
      "Epoch 189: val_loss improved from 0.36544 to 0.36480, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.36480\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.36480\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.36480\n",
      "\n",
      "Epoch 193: val_loss improved from 0.36480 to 0.36446, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.36446 to 0.36278, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.36278\n",
      "\n",
      "Epoch 219: val_loss improved from 0.36278 to 0.36016, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.36016\n",
      "\n",
      "Epoch 233: val_loss improved from 0.36016 to 0.35978, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.35978 to 0.35181, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.35181 to 0.34432, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.34432 to 0.34375, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.34375 to 0.34303, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.34303\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.34303\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3430 - accuracy: 0.8889\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09587, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09587 to 1.09285, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09285 to 1.08969, saving model to /best_model\\model_8.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 1.08969 to 1.08709, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08709 to 1.08445, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08445 to 1.08144, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08144 to 1.07829, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07829 to 1.07486, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07486 to 1.07125, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07125 to 1.06751, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06751 to 1.06376, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06376 to 1.05933, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05933 to 1.05470, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.05470 to 1.04999, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.04999 to 1.04524, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.04524 to 1.04022, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.04022 to 1.03533, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.03533 to 1.03003, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.03003 to 1.02464, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.02464 to 1.01887, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.01887 to 1.01306, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.01306 to 1.00725, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.00725 to 1.00137, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.00137 to 0.99503, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.99503 to 0.98810, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.98810 to 0.98111, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98111 to 0.97263, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.97263 to 0.96319, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.96319 to 0.95256, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.95256 to 0.94082, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.94082 to 0.92762, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.92762 to 0.91272, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.91272 to 0.89677, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.89677 to 0.88104, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.88104 to 0.86568, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.86568 to 0.85098, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.85098 to 0.83681, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.83681 to 0.82293, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.82293 to 0.80849, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.80849 to 0.79372, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.79372 to 0.78022, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.78022 to 0.76846, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.76846 to 0.75704, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.75704 to 0.74984, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.74984 to 0.74450, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.74450 to 0.73687, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.73687 to 0.72719, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.72719 to 0.71753, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.71753 to 0.71225, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.71225 to 0.70838, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.70838 to 0.70270, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.70270 to 0.69835, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.69835 to 0.69307, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.69307 to 0.68857, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.68857 to 0.68499, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.68499 to 0.68254, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.68254 to 0.67882, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.67882 to 0.67520, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.67520 to 0.67241, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.67241 to 0.67030, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.67030 to 0.66780, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.66780 to 0.66131, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.66131 to 0.65619, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.65619 to 0.65140, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.65140 to 0.64772, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.64772 to 0.64672, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.64672 to 0.64536, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.64536 to 0.64310, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.64310 to 0.64170, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.64170\n",
      "\n",
      "Epoch 71: val_loss improved from 0.64170 to 0.64160, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.64160 to 0.64148, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.64148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 111: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.64148\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.64148\n",
      "Epoch 222: early stopping\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6415 - accuracy: 0.7778\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09057, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09057 to 1.08400, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08400 to 1.07659, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.07659 to 1.06930, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.06930 to 1.06170, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.06170 to 1.05408, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.05408 to 1.04643, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.04643 to 1.03856, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.03856 to 1.03113, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.03113 to 1.02391, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.02391 to 1.01781, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.01781 to 1.01120, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.01120 to 1.00524, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.00524 to 0.99986, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.99986 to 0.99449, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.99449 to 0.98939, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.98939 to 0.98495, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.98495 to 0.98061, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.98061 to 0.97670, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.97670 to 0.97257, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.97257 to 0.96792, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.96792 to 0.96263, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.96263 to 0.95591, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.95591 to 0.94746, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.94746 to 0.93900, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.93900 to 0.92967, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.92967 to 0.91949, saving model to /best_model\\model_9.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss improved from 0.91949 to 0.90971, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.90971 to 0.90083, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.90083 to 0.89216, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.89216 to 0.88514, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.88514 to 0.87947, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.87947 to 0.87458, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.87458 to 0.87107, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.87107 to 0.86933, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.86933 to 0.86725, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.86725 to 0.86333, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.86333 to 0.86014, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.86014 to 0.85939, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.85939 to 0.85735, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.85735 to 0.85655, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.85655 to 0.85571, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.85571\n",
      "\n",
      "Epoch 44: val_loss improved from 0.85571 to 0.85539, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.85539 to 0.85021, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.85021 to 0.84492, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.84492 to 0.84139, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.84139 to 0.84036, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.84036 to 0.83858, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.83858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.83858\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.83858\n",
      "Epoch 199: early stopping\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8386 - accuracy: 0.6667\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09494, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09494 to 1.09112, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09112 to 1.08698, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08698 to 1.08258, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08258 to 1.07791, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.07791 to 1.07254, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07254 to 1.06722, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.06722 to 1.06161, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.06161 to 1.05523, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.05523 to 1.04916, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.04916 to 1.04280, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04280 to 1.03633, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.03633 to 1.02967, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.02967 to 1.02251, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.02251 to 1.01521, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.01521 to 1.00708, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.00708 to 0.99881, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.99881 to 0.98947, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.98947 to 0.97977, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.97977 to 0.96953, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.96953 to 0.95829, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.95829 to 0.94601, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.94601 to 0.93361, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.93361 to 0.92058, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.92058 to 0.90645, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.90645 to 0.89214, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.89214 to 0.87793, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.87793 to 0.86272, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.86272 to 0.84693, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.84693 to 0.83077, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.83077 to 0.81267, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.81267 to 0.79540, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.79540 to 0.77814, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.77814 to 0.76122, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.76122 to 0.74431, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.74431 to 0.72691, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.72691 to 0.71044, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.71044 to 0.69487, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.69487 to 0.67817, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.67817 to 0.66224, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.66224 to 0.64889, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.64889 to 0.63806, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.63806 to 0.62743, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.62743 to 0.61796, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.61796 to 0.60703, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.60703 to 0.59879, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.59879 to 0.59325, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.59325 to 0.58708, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.58708 to 0.58165, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.58165 to 0.57672, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.57672 to 0.56965, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.56965 to 0.56667, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.56667 to 0.56293, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.56293\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.56293\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.56293\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.56293\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.56293\n",
      "\n",
      "Epoch 59: val_loss improved from 0.56293 to 0.55980, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.55980 to 0.55661, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.55661\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.55661\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.55661\n",
      "\n",
      "Epoch 64: val_loss improved from 0.55661 to 0.54881, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.54881 to 0.53092, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.53092 to 0.51499, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.51499 to 0.50809, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.50809 to 0.50427, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.50427\n",
      "\n",
      "Epoch 77: val_loss improved from 0.50427 to 0.49784, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.49784 to 0.48941, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.48941 to 0.48026, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.48026 to 0.47251, saving model to /best_model\\model_10.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: val_loss improved from 0.47251 to 0.46916, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.46916\n",
      "\n",
      "Epoch 88: val_loss improved from 0.46916 to 0.46260, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.46260 to 0.45566, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.45566 to 0.45345, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.45345\n",
      "\n",
      "Epoch 92: val_loss improved from 0.45345 to 0.45030, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.45030 to 0.44970, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.44970\n",
      "\n",
      "Epoch 101: val_loss improved from 0.44970 to 0.44684, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.44684 to 0.44568, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.44568 to 0.44517, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.44517 to 0.44373, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.44373 to 0.44355, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.44355 to 0.43780, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.43780 to 0.43286, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.43286 to 0.43213, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.43213 to 0.43039, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.43039 to 0.42402, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.42402 to 0.41768, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.41768 to 0.41566, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.41566\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.41566\n",
      "\n",
      "Epoch 115: val_loss improved from 0.41566 to 0.41203, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.41203 to 0.41124, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.41124\n",
      "\n",
      "Epoch 125: val_loss improved from 0.41124 to 0.40894, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.40894 to 0.40557, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.40557 to 0.40404, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.40404 to 0.40173, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.40173\n",
      "\n",
      "Epoch 136: val_loss improved from 0.40173 to 0.39801, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.39801 to 0.39471, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.39471 to 0.39197, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.39197\n",
      "\n",
      "Epoch 155: val_loss improved from 0.39197 to 0.39015, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.39015 to 0.38679, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.38679 to 0.38341, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.38341 to 0.38314, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.38314\n",
      "\n",
      "Epoch 160: val_loss improved from 0.38314 to 0.37956, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.37956 to 0.37314, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.37314 to 0.36689, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.36689 to 0.36218, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.36218 to 0.36206, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.36206\n",
      "\n",
      "Epoch 166: val_loss improved from 0.36206 to 0.35768, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.35768\n",
      "\n",
      "Epoch 196: val_loss improved from 0.35768 to 0.35719, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.35719 to 0.35403, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.35403 to 0.35169, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.35169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 210: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.35169\n",
      "\n",
      "Epoch 236: val_loss improved from 0.35169 to 0.35117, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.35117 to 0.34449, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.34449 to 0.34272, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.34272\n",
      "\n",
      "Epoch 246: val_loss improved from 0.34272 to 0.34048, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.34048\n",
      "\n",
      "Epoch 268: val_loss improved from 0.34048 to 0.33952, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.33952 to 0.33685, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.33685\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.33685\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3368 - accuracy: 0.8889\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 앞에 keras 관련 import문이랑 사용자정의함수 돌리고 실행하기\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix # 혼동행렬\n",
    "\n",
    "k = 10\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "acc_score = [] # 정확도를 정리할 리스트\n",
    "cf_matrix = [] # 혼동행렬을 정리할 리스트\n",
    "\n",
    "\n",
    "fold_val = 1\n",
    "for train_index, test_index in kfold.split(x, y.argmax(axis=1)) : \n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # create new model\n",
    "    model = model_fn(1) # feature_num\n",
    "    # compile new model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=150) \n",
    "    mc = ModelCheckpoint(get_model_name(fold_val), monitor='val_loss', mode='min',  \n",
    "                         save_best_only=True, verbose = 1)\n",
    "    history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=300, verbose=0,\n",
    "                       callbacks=[es, mc], validation_data=(x_test, y_test))\n",
    "    \n",
    "    model.load_weights(\"/best_model/model_\"+str(fold_val)+\".h5\") \n",
    "    \n",
    "    accuracy = model.evaluate(x_test, y_test)[1]\n",
    "    acc_score.append(accuracy)\n",
    "    \n",
    "    # 혼동행렬\n",
    "    y_pred = model.predict(x_test).argmax(axis=1)\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    cf_matrix.append(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fa8be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [0.7222222089767456, 0.7222222089767456, 0.7222222089767456, 0.7777777910232544, 0.7777777910232544]\n",
      "정확도 평균 : 0.7444444417953491\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "423d2a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[4, 1, 0],\n",
       "        [0, 1, 3],\n",
       "        [0, 1, 8]], dtype=int64),\n",
       " array([[5, 0, 0],\n",
       "        [1, 0, 3],\n",
       "        [1, 0, 8]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [1, 0, 4],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[3, 1, 0],\n",
       "        [0, 3, 2],\n",
       "        [1, 0, 8]], dtype=int64),\n",
       " array([[ 4,  0,  0],\n",
       "        [ 0,  0,  4],\n",
       "        [ 0,  0, 10]], dtype=int64)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68660b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2ca3599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [0.7777777910232544, 0.8888888955116272, 0.6666666865348816, 0.6666666865348816, 0.7777777910232544, 0.7777777910232544, 0.8888888955116272, 0.7777777910232544, 0.8888888955116272, 0.6666666865348816]\n",
      "정확도 평균 : 0.7777777910232544\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45f12ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3, 0, 0],\n",
       "        [0, 0, 2],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[3, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 3]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 0, 3],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 0, 2],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [1, 1, 0],\n",
       "        [0, 1, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 1, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 0, 2],\n",
       "        [1, 0, 4]], dtype=int64)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdeda05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98da6f4",
   "metadata": {},
   "source": [
    "# 모델 C (betti curve argmax + persistence landscape argmax 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bc6cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [curve_time1[[\"curve_h1\"]], curve_time2[[\"curve_h1\"]], curve_time3[[\"curve_h1\"]],\n",
    "       curve_time4[[\"curve_h1\"]], curve_time5[[\"curve_h1\"]], curve_time6[[\"curve_h1\"]]]\n",
    "curve_x = [] \n",
    "\n",
    "for i in range(90) :\n",
    "    sample_x = [] \n",
    "    for j in range(6) :\n",
    "        sample_x.append(dfs[j].iloc[i])    \n",
    "    curve_x.append(sample_x)\n",
    "\n",
    "curve_x = np.array(curve_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "716f3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pl_time1[[\"pl_h1\"]], pl_time2[[\"pl_h1\"]], pl_time3[[\"pl_h1\"]],\n",
    "       pl_time4[[\"pl_h1\"]], pl_time5[[\"pl_h1\"]], pl_time6[[\"pl_h1\"]]]\n",
    "pl_x = [] \n",
    "\n",
    "for i in range(90) :\n",
    "    sample_x = [] \n",
    "    for j in range(6) :\n",
    "        sample_x.append(dfs[j].iloc[i])    \n",
    "    pl_x.append(sample_x)\n",
    "\n",
    "pl_x = np.array(pl_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d91834e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 0 : lt, 1 : rt, 2 : hc\n",
    "y = np.array(np.repeat([0,1,2], [22, 22, 46], axis=0)) \n",
    "y = y.reshape(90,-1)\n",
    "\n",
    "# 정답 데이터 one-hot vector 처리\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dafa539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 6, 1)\n",
      "(90, 6, 1)\n",
      "(90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(curve_x.shape)\n",
    "print(pl_x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# (90, 6, feature 수)\n",
    "# (90, group 수(=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d80d2",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "562b401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "072ddf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, LSTM, Dense, Dropout, Flatten, Embedding, TimeDistributed, Reshape, Concatenate\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae5102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embednet(shape): # shape=3\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=81, output_dim=2, input_length=shape))\n",
    "    model.add(Reshape(target_shape=(-1,)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83404003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, concatenate, Dense\n",
    "\n",
    "def model_fn2():\n",
    "    input_var_curve = Input(shape=(6,1), name='input_var_curve')\n",
    "    input_var_pl = Input(shape=(6,1), name='input_var_pl')\n",
    "\n",
    "    # Embedding layer \n",
    "    embedding_curve_layer = TimeDistributed(build_embednet(1), input_shape=(6, 1))(input_var_curve)\n",
    "    embedding_pl_layer = TimeDistributed(build_embednet(1), input_shape=(6, 1))(input_var_pl)\n",
    "\n",
    "    # 다시 연결\n",
    "    concatenated_vars = concatenate([embedding_curve_layer, embedding_pl_layer])\n",
    "\n",
    "    # GRU layer\n",
    "    gru_layer1 = GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu', return_sequences=True)(concatenated_vars)\n",
    "    gru_layer2 = GRU(units=8, dropout=0.15, recurrent_dropout=0.15, activation='relu')(gru_layer1)\n",
    "\n",
    "    # Dense layers\n",
    "    dense_layer1 = Dense(16, activation='relu')(gru_layer2)\n",
    "    dropout_layer1 = Dropout(.15)(dense_layer1)\n",
    "    dense_layer2 = Dense(16, activation='relu')(dropout_layer1)\n",
    "    dropout_layer2 = Dropout(.15)(dense_layer2)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(3, activation='softmax')(dropout_layer2)\n",
    "\n",
    "    model = Model(inputs=[input_var_curve, input_var_pl], outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20dbade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_var_curve (InputLayer)   [(None, 6, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " input_var_pl (InputLayer)      [(None, 6, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 6, 2)        162         ['input_var_curve[0][0]']        \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 6, 2)        162         ['input_var_pl[0][0]']           \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 6, 4)         0           ['time_distributed_1[0][0]',     \n",
      "                                                                  'time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 6, 8)         336         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 8)            432         ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           144         ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 16)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            51          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,559\n",
      "Trainable params: 1,559\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_fn2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d2881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fe70f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k겹 때마다 가장 좋은 모델을 저장하기 위한 파일 이름\n",
    "def get_model_name(k):\n",
    "    return '/best_model/model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e97e237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09820, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09820 to 1.09702, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09702 to 1.09611, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09611 to 1.09526, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09526 to 1.09446, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.09446 to 1.09318, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.09318 to 1.09205, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.09205 to 1.09084, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.09084 to 1.08968, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08968 to 1.08842, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.08842 to 1.08707, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08707 to 1.08523, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.08523 to 1.08355, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.08355 to 1.08181, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.08181 to 1.08013, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.08013 to 1.07786, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.07786 to 1.07511, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.07511 to 1.07228, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.07228 to 1.06891, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.06891 to 1.06513, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.06513 to 1.06124, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.06124 to 1.05672, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.05672 to 1.05099, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.05099 to 1.04448, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.04448 to 1.03737, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.03737 to 1.02882, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.02882 to 1.01970, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 1.01970 to 1.00952, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 1.00952 to 0.99779, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.99779 to 0.98524, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.98524 to 0.97111, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.97111 to 0.95551, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.95551 to 0.93805, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.93805 to 0.91925, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.91925 to 0.89847, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.89847 to 0.87686, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.87686 to 0.85332, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.85332 to 0.82837, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.82837 to 0.80375, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.80375 to 0.77890, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.77890 to 0.75289, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.75289 to 0.73100, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.73100 to 0.71704, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.71704 to 0.71115, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.71115 to 0.69969, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.69969 to 0.68462, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.68462 to 0.67465, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.67465 to 0.65450, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.65450 to 0.63960, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.63960 to 0.62037, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.62037 to 0.60412, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.60412 to 0.58940, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.58940 to 0.56644, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.56644 to 0.54327, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.54327 to 0.51976, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.51976 to 0.49366, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.49366 to 0.47444, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.47444 to 0.45108, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.45108 to 0.42140, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.42140 to 0.40182, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.40182 to 0.37276, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.37276 to 0.34672, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.34672 to 0.32457, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.32457\n",
      "\n",
      "Epoch 65: val_loss improved from 0.32457 to 0.31275, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.31275 to 0.26654, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.26654 to 0.22622, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.22622 to 0.21447, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.21447 to 0.20043, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.20043 to 0.16767, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.16767 to 0.14755, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.14755 to 0.13666, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.13666 to 0.13075, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.13075 to 0.10794, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.10794 to 0.10362, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.10362 to 0.07308, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.07308 to 0.05954, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.05954 to 0.05325, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.05325 to 0.04483, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.04483 to 0.03534, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.03534 to 0.03277, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.03277\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.03277\n",
      "\n",
      "Epoch 84: val_loss improved from 0.03277 to 0.02626, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.02626 to 0.02439, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.02439 to 0.02312, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.02312\n",
      "\n",
      "Epoch 88: val_loss improved from 0.02312 to 0.02217, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.02217 to 0.02097, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.02097 to 0.02045, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.02045 to 0.02012, saving model to /best_model\\model_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92: val_loss did not improve from 0.02012\n",
      "\n",
      "Epoch 93: val_loss improved from 0.02012 to 0.01966, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.01966 to 0.01861, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.01861\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.01861\n",
      "\n",
      "Epoch 97: val_loss improved from 0.01861 to 0.01844, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.01844 to 0.01775, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.01775 to 0.01612, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.01612 to 0.01492, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.01492 to 0.01409, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.01409 to 0.01359, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.01359\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.01359\n",
      "\n",
      "Epoch 105: val_loss improved from 0.01359 to 0.01242, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.01242 to 0.01088, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.01088 to 0.00989, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.00989 to 0.00912, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.00912 to 0.00879, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.00879 to 0.00852, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 114: val_loss improved from 0.00852 to 0.00845, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.00845 to 0.00793, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.00793 to 0.00691, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.00691 to 0.00631, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.00631 to 0.00608, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.00608 to 0.00587, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.00587 to 0.00561, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.00561 to 0.00528, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.00528 to 0.00521, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 129: val_loss improved from 0.00521 to 0.00513, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.00513 to 0.00469, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.00469 to 0.00435, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.00435 to 0.00416, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.00416 to 0.00404, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.00404 to 0.00402, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 140: val_loss improved from 0.00402 to 0.00389, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.00389 to 0.00343, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.00343 to 0.00301, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.00301 to 0.00271, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.00271 to 0.00246, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.00246 to 0.00230, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 152: val_loss improved from 0.00230 to 0.00189, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.00189 to 0.00173, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.00173 to 0.00172, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.00172 to 0.00165, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00165 to 0.00162, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 165: val_loss improved from 0.00162 to 0.00158, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00158 to 0.00157, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 187: val_loss improved from 0.00157 to 0.00155, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.00155 to 0.00154, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.00154 to 0.00142, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00142 to 0.00133, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.00133 to 0.00130, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 193: val_loss improved from 0.00130 to 0.00127, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00127 to 0.00124, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.00124 to 0.00116, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00116 to 0.00105, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 212: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00105\n",
      "\n",
      "Epoch 225: val_loss improved from 0.00105 to 0.00102, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.00102 to 0.00100, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00100\n",
      "\n",
      "Epoch 236: val_loss improved from 0.00100 to 0.00096, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.00096 to 0.00078, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.00078 to 0.00070, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00070\n",
      "\n",
      "Epoch 278: val_loss improved from 0.00070 to 0.00070, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.00070 to 0.00067, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.00067 to 0.00067, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.00067 to 0.00066, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.00066 to 0.00065, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.00065 to 0.00061, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.00061 to 0.00059, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.00059 to 0.00057, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.00057 to 0.00055, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.00055 to 0.00054, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 296: val_loss improved from 0.00054 to 0.00054, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.00054 to 0.00054, saving model to /best_model\\model_1.h5\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00054\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00054\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.3909e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09540, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09540 to 1.09282, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09282 to 1.09029, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09029 to 1.08732, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08732 to 1.08394, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08394 to 1.08035, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08035 to 1.07737, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07737 to 1.07442, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07442 to 1.07135, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07135 to 1.06824, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06824 to 1.06523, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06523 to 1.06264, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06264 to 1.06024, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06024 to 1.05775, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05775 to 1.05494, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05494 to 1.05205, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05205 to 1.04917, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.04917 to 1.04618, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04618 to 1.04314, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.04314 to 1.03985, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.03985 to 1.03612, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.03612 to 1.03156, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.03156 to 1.02624, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.02624 to 1.01979, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.01979 to 1.01247, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 1.01247 to 1.00454, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 1.00454 to 0.99513, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.99513 to 0.98370, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.98370 to 0.97140, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.97140 to 0.95777, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.95777 to 0.94214, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss improved from 0.94214 to 0.92518, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.92518 to 0.90798, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.90798 to 0.89000, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.89000 to 0.86910, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.86910 to 0.84774, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.84774 to 0.83151, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.83151 to 0.81638, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.81638 to 0.80289, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.80289 to 0.79008, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.79008 to 0.77882, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.77882 to 0.76786, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.76786 to 0.75995, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.75995 to 0.75392, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.75392 to 0.74973, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.74973 to 0.74773, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.74773 to 0.74677, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.74677 to 0.74599, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.74599 to 0.74501, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.74501 to 0.74404, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.74404 to 0.74290, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.74290 to 0.74110, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.74110 to 0.73889, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.73889 to 0.73680, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.73680 to 0.73434, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.73434 to 0.73277, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.73277 to 0.73151, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.73151 to 0.72830, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.72830 to 0.72571, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.72571 to 0.72329, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.72329 to 0.72232, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.72232 to 0.72102, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.72102 to 0.71972, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.71972 to 0.71926, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.71926\n",
      "\n",
      "Epoch 72: val_loss improved from 0.71926 to 0.71846, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.71846 to 0.71583, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.71583 to 0.71130, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.71130 to 0.70564, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.70564 to 0.70283, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.70283 to 0.69940, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.69940 to 0.69623, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.69623 to 0.69250, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.69250 to 0.68844, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.68844 to 0.68551, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.68551 to 0.68468, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.68468 to 0.68318, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.68318 to 0.67871, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.67871 to 0.67372, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.67372 to 0.67282, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.67282 to 0.66646, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.66646 to 0.65760, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.65760 to 0.65432, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.65432 to 0.64969, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.64969 to 0.64281, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.64281 to 0.63340, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.63340 to 0.62015, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.62015 to 0.60204, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.60204 to 0.58582, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.58582 to 0.56790, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.56790 to 0.55565, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.55565 to 0.54623, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.54623 to 0.53434, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.53434 to 0.52468, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.52468 to 0.51797, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.51797 to 0.50752, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.50752 to 0.49724, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.49724 to 0.49031, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.49031 to 0.47689, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.47689 to 0.45474, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.45474 to 0.44649, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.44649\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.44649\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.44649\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.44649\n",
      "\n",
      "Epoch 112: val_loss improved from 0.44649 to 0.42480, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.42480 to 0.37814, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.37814 to 0.32852, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.32852 to 0.30548, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.30548 to 0.29948, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.29948\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.29948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: val_loss improved from 0.29948 to 0.26986, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.26986 to 0.18259, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.18259\n",
      "\n",
      "Epoch 136: val_loss improved from 0.18259 to 0.17581, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.17581 to 0.14617, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.14617\n",
      "\n",
      "Epoch 156: val_loss improved from 0.14617 to 0.09554, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09554\n",
      "\n",
      "Epoch 166: val_loss improved from 0.09554 to 0.09182, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.09182 to 0.05310, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.05310 to 0.03938, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.03938 to 0.03767, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.03767\n",
      "\n",
      "Epoch 285: val_loss improved from 0.03767 to 0.03683, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.03683 to 0.00630, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.00630 to 0.00110, saving model to /best_model\\model_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 288: val_loss improved from 0.00110 to 0.00056, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.00056 to 0.00055, saving model to /best_model\\model_2.h5\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00055\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5.5217e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09686, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09686 to 1.09514, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09514 to 1.09347, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09347 to 1.09177, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.09177 to 1.08958, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08958 to 1.08743, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08743 to 1.08529, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08529 to 1.08311, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.08311 to 1.08067, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.08067 to 1.07818, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07818 to 1.07548, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.07548 to 1.07278, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.07278 to 1.06979, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06979 to 1.06643, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.06643 to 1.06254, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.06254 to 1.05805, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05805 to 1.05307, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.05307 to 1.04712, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04712 to 1.04096, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.04096 to 1.03451, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.03451 to 1.02769, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.02769 to 1.02032, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.02032 to 1.01232, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.01232 to 1.00305, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.00305 to 0.99246, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99246 to 0.97924, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.97924 to 0.96381, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.96381 to 0.94750, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.94750 to 0.93001, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.93001 to 0.91183, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.91183 to 0.89011, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.89011 to 0.86446, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.86446 to 0.83733, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.83733 to 0.80956, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.80956 to 0.78471, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.78471 to 0.76314, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.76314 to 0.74472, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.74472 to 0.72884, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.72884 to 0.71399, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.71399 to 0.69809, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.69809 to 0.68603, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.68603 to 0.68184, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.68184 to 0.66685, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.66685 to 0.65277, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.65277 to 0.64371, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.64371 to 0.63534, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.63534 to 0.62675, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.62675 to 0.61849, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.61849 to 0.60062, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.60062 to 0.58305, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.58305 to 0.57306, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.57306 to 0.56443, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.56443 to 0.55202, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.55202 to 0.53836, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.53836 to 0.52087, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.52087 to 0.51022, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.51022 to 0.50560, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.50560\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.50560\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.50560\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.50560\n",
      "\n",
      "Epoch 62: val_loss improved from 0.50560 to 0.48891, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.48891 to 0.47288, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.47288 to 0.45606, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.45606 to 0.45430, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.45430 to 0.45140, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.45140 to 0.44811, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.44811 to 0.44640, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.44640 to 0.44521, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.44521 to 0.44512, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.44512 to 0.44409, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.44409\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.44409\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.44409\n",
      "\n",
      "Epoch 75: val_loss improved from 0.44409 to 0.43929, saving model to /best_model\\model_3.h5\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.43929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.43929\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.43929\n",
      "Epoch 225: early stopping\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4393 - accuracy: 0.7778\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09549, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09549 to 1.09279, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09279 to 1.09026, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09026 to 1.08777, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08777 to 1.08506, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08506 to 1.08254, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08254 to 1.07989, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07989 to 1.07715, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07715 to 1.07440, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07440 to 1.07143, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07143 to 1.06849, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06849 to 1.06543, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06543 to 1.06210, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06210 to 1.05839, saving model to /best_model\\model_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 1.05839 to 1.05455, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05455 to 1.05040, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05040 to 1.04592, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.04592 to 1.04100, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04100 to 1.03539, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.03539 to 1.02916, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.02916 to 1.02238, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.02238 to 1.01412, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.01412 to 1.00438, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.00438 to 0.99232, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.99232 to 0.97831, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.97831 to 0.96166, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.96166 to 0.94223, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.94223 to 0.91999, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.91999 to 0.89637, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.89637 to 0.87161, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.87161 to 0.84724, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.84724 to 0.82411, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.82411 to 0.79950, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.79950 to 0.77640, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.77640 to 0.75624, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.75624 to 0.73423, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.73423 to 0.71281, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.71281 to 0.69291, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.69291 to 0.67477, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.67477 to 0.66114, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.66114 to 0.65092, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.65092 to 0.64180, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.64180 to 0.63330, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.63330 to 0.62570, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.62570 to 0.61894, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.61894 to 0.61328, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.61328 to 0.60781, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.60781 to 0.60203, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.60203 to 0.59702, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.59702 to 0.59275, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.59275 to 0.58898, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.58898 to 0.58603, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.58603 to 0.58105, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.58105 to 0.57412, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.57412 to 0.56655, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.56655 to 0.55925, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.55925 to 0.55313, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.55313 to 0.54628, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.54628 to 0.53809, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.53809 to 0.52931, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.52931 to 0.52008, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.52008 to 0.51185, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.51185 to 0.50218, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.50218 to 0.49191, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.49191 to 0.47707, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.47707 to 0.45776, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.45776 to 0.44006, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.44006 to 0.42481, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.42481 to 0.40964, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.40964 to 0.39220, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.39220 to 0.37539, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.37539 to 0.36014, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.36014 to 0.34457, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.34457 to 0.32860, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.32860 to 0.30649, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.30649 to 0.27941, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.27941 to 0.26281, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.26281\n",
      "\n",
      "Epoch 79: val_loss improved from 0.26281 to 0.26219, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.26219 to 0.24013, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.24013 to 0.21266, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.21266 to 0.18603, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.18603 to 0.17644, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.17644\n",
      "\n",
      "Epoch 85: val_loss improved from 0.17644 to 0.16272, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16272\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16272\n",
      "\n",
      "Epoch 88: val_loss improved from 0.16272 to 0.15432, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.15432 to 0.13303, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.13303 to 0.11979, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.11979 to 0.11597, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.11597 to 0.11091, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.11091 to 0.10228, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.10228 to 0.09210, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.09210 to 0.08412, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.08412 to 0.07827, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.07827 to 0.07190, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.07190 to 0.06476, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.06476 to 0.05965, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.05965 to 0.05766, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.05766 to 0.05515, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.05515 to 0.05453, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05453\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05453\n",
      "\n",
      "Epoch 105: val_loss improved from 0.05453 to 0.05061, saving model to /best_model\\model_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: val_loss improved from 0.05061 to 0.03981, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.03981 to 0.03617, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.03617 to 0.03540, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.03540 to 0.03322, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.03322 to 0.03151, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.03151\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.03151\n",
      "\n",
      "Epoch 113: val_loss improved from 0.03151 to 0.03001, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.03001 to 0.02745, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.02745 to 0.02519, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.02519 to 0.02393, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.02393 to 0.02294, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.02294\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.02294\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.02294\n",
      "\n",
      "Epoch 121: val_loss improved from 0.02294 to 0.02229, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.02229 to 0.02022, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.02022 to 0.01837, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.01837 to 0.01700, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.01700 to 0.01595, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.01595 to 0.01464, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.01464 to 0.01358, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.01358 to 0.01257, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.01257 to 0.01169, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01169\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01169\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01169\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01169 to 0.01169, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.01169 to 0.01104, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.01104 to 0.01060, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.01060 to 0.01002, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01002 to 0.00988, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.00988\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.00988\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.00988\n",
      "\n",
      "Epoch 141: val_loss improved from 0.00988 to 0.00830, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.00830 to 0.00705, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.00705 to 0.00696, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.00696 to 0.00682, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00682\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00682 to 0.00632, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.00632 to 0.00573, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00573\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00573\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00573 to 0.00550, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.00550 to 0.00525, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00525 to 0.00519, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00519\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00519 to 0.00512, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00512\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00512\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00512\n",
      "\n",
      "Epoch 170: val_loss improved from 0.00512 to 0.00492, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00492 to 0.00448, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00448\n",
      "\n",
      "Epoch 180: val_loss improved from 0.00448 to 0.00447, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.00447 to 0.00429, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.00429 to 0.00416, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00416 to 0.00407, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.00407 to 0.00400, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00400 to 0.00398, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00398\n",
      "\n",
      "Epoch 200: val_loss improved from 0.00398 to 0.00386, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.00386 to 0.00368, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00368 to 0.00356, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.00356 to 0.00353, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00353 to 0.00349, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00349 to 0.00330, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.00330 to 0.00309, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.00309 to 0.00282, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00282 to 0.00257, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00257 to 0.00239, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.00239 to 0.00222, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.00222 to 0.00209, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.00209 to 0.00192, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.00192 to 0.00177, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.00177 to 0.00164, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.00164 to 0.00163, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 219: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 241: val_loss improved from 0.00163 to 0.00159, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.00159 to 0.00152, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.00152 to 0.00152, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00152\n",
      "\n",
      "Epoch 252: val_loss improved from 0.00152 to 0.00147, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.00147 to 0.00124, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.00124 to 0.00116, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00116\n",
      "\n",
      "Epoch 282: val_loss improved from 0.00116 to 0.00111, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.00111 to 0.00109, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.00109 to 0.00106, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.00106 to 0.00102, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.00102 to 0.00098, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.00098 to 0.00092, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.00092 to 0.00085, saving model to /best_model\\model_4.h5\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00085\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00085\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.5399e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09602, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09602 to 1.09364, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09364 to 1.09124, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09124 to 1.08869, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08869 to 1.08627, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08627 to 1.08385, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08385 to 1.08130, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.08130 to 1.07859, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07859 to 1.07567, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07567 to 1.07237, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.07237 to 1.06906, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06906 to 1.06604, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06604 to 1.06243, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.06243 to 1.05846, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.05846 to 1.05450, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.05450 to 1.05051, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.05051 to 1.04671, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.04671 to 1.04220, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.04220 to 1.03685, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 1.03685 to 1.03065, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 1.03065 to 1.02434, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 1.02434 to 1.01751, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 1.01751 to 1.01018, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 1.01018 to 1.00204, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 1.00204 to 0.99316, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.99316 to 0.98303, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.98303 to 0.97145, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.97145 to 0.95874, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.95874 to 0.94675, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.94675 to 0.93282, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.93282 to 0.91667, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.91667 to 0.89985, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.89985 to 0.88243, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.88243 to 0.86357, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.86357 to 0.84462, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.84462 to 0.82665, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.82665 to 0.80869, saving model to /best_model\\model_5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss improved from 0.80869 to 0.79126, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.79126 to 0.77511, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.77511 to 0.76030, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.76030 to 0.74664, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.74664 to 0.73483, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.73483 to 0.72222, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.72222 to 0.71214, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.71214 to 0.70264, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.70264 to 0.69385, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.69385 to 0.68424, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.68424 to 0.67374, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.67374 to 0.66380, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.66380 to 0.65168, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.65168 to 0.63866, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.63866 to 0.62437, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.62437 to 0.60932, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.60932 to 0.59353, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.59353 to 0.57631, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.57631 to 0.55323, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.55323 to 0.52203, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.52203 to 0.49223, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.49223 to 0.46443, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.46443 to 0.43513, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.43513 to 0.40623, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.40623 to 0.37863, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.37863 to 0.35507, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.35507 to 0.35217, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.35217 to 0.33647, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.33647 to 0.30433, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.30433 to 0.28233, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.28233 to 0.26628, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.26628 to 0.25640, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.25640\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.25640\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.25640\n",
      "\n",
      "Epoch 73: val_loss improved from 0.25640 to 0.24305, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.24305 to 0.21227, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.21227 to 0.19723, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.19723 to 0.19056, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.19056 to 0.18309, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.18309 to 0.17347, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.17347 to 0.16620, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.16620 to 0.15753, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.15753 to 0.14427, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.14427 to 0.13045, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.13045 to 0.12378, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.12378\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.12378\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.12378\n",
      "\n",
      "Epoch 87: val_loss improved from 0.12378 to 0.10026, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.10026 to 0.08675, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.08675 to 0.08180, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.08180\n",
      "\n",
      "Epoch 102: val_loss improved from 0.08180 to 0.07416, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.07416 to 0.07180, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 122: val_loss improved from 0.07180 to 0.05347, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.05347 to 0.04628, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.04628\n",
      "\n",
      "Epoch 147: val_loss improved from 0.04628 to 0.03980, saving model to /best_model\\model_5.h5\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.03980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.03980\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.03980\n",
      "Epoch 297: early stopping\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09747, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09747 to 1.09382, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09382 to 1.09005, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09005 to 1.08577, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08577 to 1.08109, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08109 to 1.07622, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07622 to 1.07127, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07127 to 1.06579, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.06579 to 1.06017, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.06017 to 1.05429, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.05429 to 1.04828, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04828 to 1.04170, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: val_loss improved from 1.04170 to 1.03480, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.03480 to 1.02805, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.02805 to 1.02091, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.02091 to 1.01311, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.01311 to 1.00578, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.00578 to 0.99754, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.99754 to 0.98885, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98885 to 0.97925, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.97925 to 0.96981, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.96981 to 0.95851, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.95851 to 0.94643, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.94643 to 0.93342, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.93342 to 0.91866, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.91866 to 0.90357, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.90357 to 0.88877, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.88877 to 0.87066, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.87066 to 0.84862, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.84862 to 0.82647, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.82647 to 0.80438, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.80438 to 0.78062, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.78062 to 0.75652, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.75652 to 0.73165, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.73165 to 0.70891, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.70891 to 0.68241, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.68241 to 0.65329, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.65329 to 0.62782, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.62782 to 0.60926, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.60926 to 0.59293, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.59293 to 0.58242, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.58242 to 0.57640, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.57640 to 0.56954, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.56954 to 0.56168, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.56168 to 0.55079, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.55079 to 0.53878, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.53878 to 0.52735, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.52735 to 0.51723, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.51723 to 0.50858, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.50858 to 0.50226, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.50226 to 0.49926, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.49926 to 0.49677, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.49677 to 0.49483, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.49483 to 0.49092, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.49092 to 0.48656, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.48656 to 0.48097, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.48097 to 0.47590, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.47590 to 0.47169, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.47169 to 0.46773, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.46773 to 0.46433, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.46433 to 0.46117, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.46117 to 0.45807, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.45807 to 0.45376, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.45376 to 0.44710, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.44710 to 0.43852, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.43852 to 0.43098, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.43098 to 0.42617, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.42617\n",
      "\n",
      "Epoch 69: val_loss improved from 0.42617 to 0.42520, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.42520 to 0.41356, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.41356 to 0.39709, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.39709 to 0.38698, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.38698 to 0.37685, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.37685\n",
      "\n",
      "Epoch 75: val_loss improved from 0.37685 to 0.37541, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.37541 to 0.35550, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.35550 to 0.33554, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.33554 to 0.31293, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.31293 to 0.29540, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.29540 to 0.29299, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.29299\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.29299\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.29299\n",
      "\n",
      "Epoch 84: val_loss improved from 0.29299 to 0.27240, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.27240 to 0.25144, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.25144 to 0.23650, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.23650 to 0.22204, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.22204 to 0.21471, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.21471 to 0.21361, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.21361\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.21361\n",
      "\n",
      "Epoch 92: val_loss improved from 0.21361 to 0.19447, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.19447 to 0.16610, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.16610 to 0.14475, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.14475 to 0.13771, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.13771 to 0.12856, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.12856 to 0.11318, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.11318 to 0.10177, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.10177 to 0.09706, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09706\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09706\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09706\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09706\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09706\n",
      "\n",
      "Epoch 105: val_loss improved from 0.09706 to 0.08738, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.08738 to 0.06772, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 107: val_loss improved from 0.06772 to 0.05845, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.05845 to 0.05544, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.05544 to 0.05263, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.05263 to 0.05198, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.05198 to 0.05168, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.05168 to 0.04892, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.04892 to 0.04501, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.04501 to 0.04094, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.04094 to 0.03940, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.03940 to 0.03701, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.03701 to 0.03662, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.03662\n",
      "\n",
      "Epoch 120: val_loss improved from 0.03662 to 0.03434, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.03434 to 0.03205, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.03205 to 0.03107, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.03107 to 0.02937, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.02937\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.02937\n",
      "\n",
      "Epoch 126: val_loss improved from 0.02937 to 0.02558, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.02558 to 0.02157, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.02157 to 0.02050, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.02050 to 0.01982, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.01982\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.01982\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.01982\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.01982\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01982\n",
      "\n",
      "Epoch 135: val_loss improved from 0.01982 to 0.01817, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.01817 to 0.01737, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01737 to 0.01677, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.01677 to 0.01602, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.01602 to 0.01544, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.01544 to 0.01458, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.01458 to 0.01393, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.01393 to 0.01362, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.01362 to 0.01315, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.01315\n",
      "\n",
      "Epoch 145: val_loss improved from 0.01315 to 0.01295, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.01295 to 0.01228, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.01228 to 0.01166, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.01166 to 0.01100, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.01100 to 0.00967, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.00967 to 0.00888, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.00888 to 0.00837, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.00837 to 0.00797, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00797 to 0.00782, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.00782 to 0.00749, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00749 to 0.00724, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.00724 to 0.00684, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.00684 to 0.00660, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00660\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00660 to 0.00641, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.00641 to 0.00633, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00633 to 0.00619, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.00619 to 0.00583, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00583 to 0.00530, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00530 to 0.00501, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.00501 to 0.00493, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.00493 to 0.00484, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.00484 to 0.00479, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00479 to 0.00471, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00471 to 0.00457, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.00457 to 0.00433, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00433\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00433 to 0.00419, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00419 to 0.00395, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.00395 to 0.00379, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.00379 to 0.00373, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00373 to 0.00372, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00372\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00372\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.00372\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.00372\n",
      "\n",
      "Epoch 188: val_loss improved from 0.00372 to 0.00359, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.00359 to 0.00343, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00343 to 0.00329, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00329 to 0.00324, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.00324 to 0.00314, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00314\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00314\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00314\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.00314\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00314 to 0.00297, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.00297 to 0.00280, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00280 to 0.00269, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00269 to 0.00260, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.00260 to 0.00253, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.00253 to 0.00246, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00246 to 0.00241, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00241 to 0.00236, saving model to /best_model\\model_6.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 210: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 215: val_loss improved from 0.00236 to 0.00227, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.00227 to 0.00209, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.00209 to 0.00196, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.00196 to 0.00188, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.00188 to 0.00181, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.00181 to 0.00175, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.00175 to 0.00169, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.00169 to 0.00165, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.00165 to 0.00163, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 227: val_loss improved from 0.00163 to 0.00159, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.00159 to 0.00155, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.00155 to 0.00152, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.00152 to 0.00150, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.00150 to 0.00145, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.00145 to 0.00143, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00143\n",
      "\n",
      "Epoch 243: val_loss improved from 0.00143 to 0.00142, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.00142 to 0.00136, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.00136 to 0.00132, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.00132 to 0.00129, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.00129 to 0.00126, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.00126 to 0.00125, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.00125 to 0.00122, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.00122 to 0.00120, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.00120 to 0.00118, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.00118 to 0.00117, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.00117 to 0.00111, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.00111 to 0.00107, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 258: val_loss improved from 0.00107 to 0.00104, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.00104 to 0.00104, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00104\n",
      "\n",
      "Epoch 262: val_loss improved from 0.00104 to 0.00098, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.00098 to 0.00092, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.00092 to 0.00088, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.00088 to 0.00086, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.00086 to 0.00083, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.00083 to 0.00081, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.00081 to 0.00080, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.00080 to 0.00078, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.00078 to 0.00077, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.00077 to 0.00076, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 278: val_loss improved from 0.00076 to 0.00073, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.00073 to 0.00073, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.00073 to 0.00072, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.00072 to 0.00071, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 297: val_loss improved from 0.00071 to 0.00069, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.00069 to 0.00067, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.00067 to 0.00066, saving model to /best_model\\model_6.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.00066 to 0.00064, saving model to /best_model\\model_6.h5\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6.4286e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09581, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09581 to 1.09305, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09305 to 1.09037, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.09037 to 1.08752, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08752 to 1.08467, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08467 to 1.08152, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.08152 to 1.07822, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07822 to 1.07496, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.07496 to 1.07117, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.07117 to 1.06681, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.06681 to 1.06162, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.06162 to 1.05639, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05639 to 1.04985, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.04985 to 1.04262, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.04262 to 1.03434, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.03434 to 1.02517, saving model to /best_model\\model_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: val_loss improved from 1.02517 to 1.01456, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.01456 to 1.00259, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 1.00259 to 0.98888, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98888 to 0.97254, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.97254 to 0.95365, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.95365 to 0.93325, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.93325 to 0.91200, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.91200 to 0.89192, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.89192 to 0.87126, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.87126 to 0.84802, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.84802 to 0.82399, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.82399 to 0.80233, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.80233 to 0.77877, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.77877 to 0.75939, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.75939 to 0.74375, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.74375 to 0.72985, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.72985 to 0.71710, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.71710 to 0.70599, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.70599 to 0.69677, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.69677 to 0.68921, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.68921 to 0.68361, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.68361 to 0.68017, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.68017 to 0.67737, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.67737 to 0.66999, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.66999 to 0.65993, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.65993 to 0.64909, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.64909 to 0.64088, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.64088 to 0.63224, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.63224 to 0.62564, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.62564 to 0.61891, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.61891 to 0.61576, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.61576 to 0.60758, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.60758 to 0.59400, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.59400 to 0.58259, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.58259 to 0.57090, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.57090 to 0.56413, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.56413 to 0.55830, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.55830 to 0.55167, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.55167 to 0.53824, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.53824 to 0.52132, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.52132 to 0.50844, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.50844 to 0.50271, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.50271\n",
      "\n",
      "Epoch 60: val_loss improved from 0.50271 to 0.49963, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.49963 to 0.49236, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.49236 to 0.49173, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.49173 to 0.48394, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.48394 to 0.47483, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.47483 to 0.45828, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.45828 to 0.44805, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.44805 to 0.43781, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.43781 to 0.43302, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.43302 to 0.42835, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.42835 to 0.42439, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.42439 to 0.41704, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.41704\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.41704\n",
      "\n",
      "Epoch 74: val_loss improved from 0.41704 to 0.40948, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.40948 to 0.40458, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.40458 to 0.40053, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.40053 to 0.39766, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.39766\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.39766\n",
      "\n",
      "Epoch 80: val_loss improved from 0.39766 to 0.39649, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.39649 to 0.38276, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.38276 to 0.37232, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.37232 to 0.36402, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.36402 to 0.36055, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.36055\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.36055\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.36055\n",
      "\n",
      "Epoch 88: val_loss improved from 0.36055 to 0.33562, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.33562 to 0.31299, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.31299 to 0.30556, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.30556 to 0.29233, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.29233\n",
      "\n",
      "Epoch 93: val_loss improved from 0.29233 to 0.29124, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.29124 to 0.28829, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.28829 to 0.27703, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.27703 to 0.27063, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.27063\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.27063\n",
      "\n",
      "Epoch 99: val_loss improved from 0.27063 to 0.26646, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.26646 to 0.25064, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.25064 to 0.23989, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.23989 to 0.23673, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.23673\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.23673\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.23673\n",
      "\n",
      "Epoch 106: val_loss improved from 0.23673 to 0.23333, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.23333 to 0.21730, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.21730 to 0.20978, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.20978 to 0.20549, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.20549 to 0.20409, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.20409\n",
      "\n",
      "Epoch 112: val_loss improved from 0.20409 to 0.19768, saving model to /best_model\\model_7.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 113: val_loss improved from 0.19768 to 0.18651, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.18651 to 0.18120, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.18120 to 0.17900, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.17900 to 0.17518, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.17518 to 0.17396, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.17396\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.17396\n",
      "\n",
      "Epoch 120: val_loss improved from 0.17396 to 0.15964, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.15964 to 0.15641, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.15641 to 0.15343, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.15343 to 0.14876, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.14876 to 0.14291, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.14291 to 0.13713, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.13713 to 0.13340, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.13340 to 0.12897, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.12897 to 0.12471, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.12471 to 0.12057, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.12057 to 0.11668, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.11668 to 0.11375, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.11375\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.11375\n",
      "\n",
      "Epoch 134: val_loss improved from 0.11375 to 0.10701, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.10701 to 0.10267, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.10267 to 0.09995, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.09995 to 0.09979, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.09979 to 0.09859, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.09859 to 0.09288, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.09288 to 0.09046, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.09046 to 0.08495, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.08495 to 0.08293, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.08293 to 0.08273, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.08273\n",
      "\n",
      "Epoch 150: val_loss improved from 0.08273 to 0.07514, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.07514 to 0.07065, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.07065 to 0.06867, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.06867\n",
      "\n",
      "Epoch 161: val_loss improved from 0.06867 to 0.06543, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.06543 to 0.05779, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.05779 to 0.05594, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.05594\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.05594\n",
      "\n",
      "Epoch 166: val_loss improved from 0.05594 to 0.05252, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.05252 to 0.05044, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.05044 to 0.04919, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.04919 to 0.04556, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.04556 to 0.03768, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.03768\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.03768\n",
      "\n",
      "Epoch 173: val_loss improved from 0.03768 to 0.03620, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.03620 to 0.03057, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.03057 to 0.02846, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.02846\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.02846\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.02846\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.02846\n",
      "\n",
      "Epoch 180: val_loss improved from 0.02846 to 0.02506, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.02506 to 0.02360, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.02360\n",
      "\n",
      "Epoch 189: val_loss improved from 0.02360 to 0.02175, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.02175 to 0.02111, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.02111\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.02111\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.02111\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.02111\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.02111\n",
      "\n",
      "Epoch 196: val_loss improved from 0.02111 to 0.01952, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.01952 to 0.01643, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.01643\n",
      "\n",
      "Epoch 216: val_loss improved from 0.01643 to 0.01562, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.01562 to 0.01514, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.01514 to 0.01509, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.01509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 233: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.01509\n",
      "\n",
      "Epoch 259: val_loss improved from 0.01509 to 0.01312, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.01312 to 0.01187, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.01187 to 0.01175, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.01175\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.01175\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.01175\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.01175\n",
      "\n",
      "Epoch 266: val_loss improved from 0.01175 to 0.01136, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.01136 to 0.01021, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.01021 to 0.00964, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.00964 to 0.00951, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00951\n",
      "\n",
      "Epoch 280: val_loss improved from 0.00951 to 0.00820, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.00820 to 0.00794, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.00794 to 0.00780, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.00780 to 0.00755, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.00755 to 0.00743, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 295: val_loss improved from 0.00743 to 0.00671, saving model to /best_model\\model_7.h5\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00671\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00671\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00671\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00671\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00671\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09386, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09386 to 1.08942, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08942 to 1.08504, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08504 to 1.08041, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08041 to 1.07565, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.07565 to 1.07093, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07093 to 1.06619, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.06619 to 1.06078, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.06078 to 1.05481, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.05481 to 1.04853, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.04853 to 1.04193, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04193 to 1.03515, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.03515 to 1.02728, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.02728 to 1.01917, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.01917 to 1.01055, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.01055 to 1.00161, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.00161 to 0.99190, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.99190 to 0.98104, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.98104 to 0.96935, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.96935 to 0.95591, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.95591 to 0.94125, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.94125 to 0.92458, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.92458 to 0.90740, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.90740 to 0.88885, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.88885 to 0.86869, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.86869 to 0.84772, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.84772 to 0.82669, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.82669 to 0.80351, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.80351 to 0.78022, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.78022 to 0.75670, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.75670 to 0.73447, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.73447 to 0.71408, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.71408 to 0.69366, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.69366 to 0.67278, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.67278 to 0.65249, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.65249 to 0.63064, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.63064 to 0.61126, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.61126 to 0.59556, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.59556 to 0.58698, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.58698 to 0.57750, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.57750 to 0.57103, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.57103 to 0.56011, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.56011 to 0.55114, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.55114 to 0.54014, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.54014 to 0.52952, saving model to /best_model\\model_8.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: val_loss improved from 0.52952 to 0.50721, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.50721 to 0.49147, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.49147 to 0.49109, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.49109\n",
      "\n",
      "Epoch 55: val_loss improved from 0.49109 to 0.48517, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.48517 to 0.46148, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.46148 to 0.43676, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.43676 to 0.39805, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.39805 to 0.38517, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.38517\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.38517\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.38517\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.38517\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.38517\n",
      "\n",
      "Epoch 65: val_loss improved from 0.38517 to 0.38382, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.38382 to 0.37413, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.37413\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.37413\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.37413\n",
      "\n",
      "Epoch 70: val_loss improved from 0.37413 to 0.37358, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.37358 to 0.35341, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.35341 to 0.34365, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.34365 to 0.33635, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.33635\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.33635\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.33635\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.33635\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.33635\n",
      "\n",
      "Epoch 79: val_loss improved from 0.33635 to 0.33392, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.33392 to 0.32213, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.32213 to 0.31898, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.31898\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.31898\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.31898\n",
      "\n",
      "Epoch 85: val_loss improved from 0.31898 to 0.29971, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.29971 to 0.26918, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.26918 to 0.25627, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.25627 to 0.25140, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.25140 to 0.24441, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.24441 to 0.24421, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.24421\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.24421\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.24421\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.24421\n",
      "\n",
      "Epoch 95: val_loss improved from 0.24421 to 0.23910, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.23910 to 0.21290, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.21290 to 0.20551, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.20551 to 0.20190, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.20190 to 0.18799, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.18799 to 0.17380, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.17380 to 0.16231, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.16231 to 0.15166, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.15166 to 0.14285, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.14285 to 0.14115, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.14115\n",
      "\n",
      "Epoch 106: val_loss improved from 0.14115 to 0.11398, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.11398 to 0.10168, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.10168 to 0.09387, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.09387 to 0.08409, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.08409 to 0.08240, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.08240\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.08240\n",
      "\n",
      "Epoch 113: val_loss improved from 0.08240 to 0.08106, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.08106 to 0.05957, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.05957 to 0.04726, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.04726 to 0.04241, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.04241 to 0.03925, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.03925 to 0.03552, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.03552 to 0.03268, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.03268\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.03268\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.03268\n",
      "\n",
      "Epoch 123: val_loss improved from 0.03268 to 0.02684, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.02684 to 0.02450, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.02450 to 0.02302, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.02302 to 0.02176, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.02176 to 0.02157, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.02157\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.02157\n",
      "\n",
      "Epoch 130: val_loss improved from 0.02157 to 0.02051, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.02051 to 0.01985, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.01985 to 0.01926, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01926 to 0.01887, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.01887 to 0.01821, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.01821 to 0.01679, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.01679 to 0.01483, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01483 to 0.01401, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.01401\n",
      "\n",
      "Epoch 139: val_loss improved from 0.01401 to 0.01360, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.01360 to 0.01192, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.01192 to 0.01089, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.01089 to 0.01005, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.01005 to 0.00951, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.00951 to 0.00896, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.00896 to 0.00845, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.00845 to 0.00819, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.00819 to 0.00811, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 152: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 153: val_loss improved from 0.00811 to 0.00755, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.00755 to 0.00699, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.00699\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00699 to 0.00661, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.00661 to 0.00609, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00609 to 0.00608, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00608 to 0.00526, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00526 to 0.00478, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.00478 to 0.00456, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00456 to 0.00449, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.00449 to 0.00433, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00433 to 0.00404, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.00404 to 0.00387, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.00387 to 0.00367, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.00367 to 0.00346, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.00346 to 0.00325, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00325 to 0.00311, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00311 to 0.00302, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.00302\n",
      "\n",
      "Epoch 174: val_loss improved from 0.00302 to 0.00283, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00283 to 0.00252, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00252 to 0.00234, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.00234 to 0.00220, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.00220 to 0.00204, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.00204 to 0.00194, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 181: val_loss improved from 0.00194 to 0.00192, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.00192 to 0.00180, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00180 to 0.00164, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.00164 to 0.00156, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00156 to 0.00143, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.00143 to 0.00127, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.00127 to 0.00113, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.00113\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.00113\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.00113\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.00113\n",
      "\n",
      "Epoch 192: val_loss improved from 0.00113 to 0.00100, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.00100 to 0.00089, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00089 to 0.00086, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.00086 to 0.00083, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00083 to 0.00081, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 201: val_loss improved from 0.00081 to 0.00079, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00079 to 0.00069, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.00069 to 0.00061, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00061 to 0.00054, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00054 to 0.00051, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00051\n",
      "\n",
      "Epoch 237: val_loss improved from 0.00051 to 0.00048, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.00048 to 0.00047, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.00047 to 0.00045, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.00045 to 0.00043, saving model to /best_model\\model_8.h5\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 278: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 300: val_loss improved from 0.00043 to 0.00039, saving model to /best_model\\model_8.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.9496e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09259, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09259 to 1.08599, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08599 to 1.07978, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.07978 to 1.07286, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.07286 to 1.06562, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.06562 to 1.05858, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.05858 to 1.05143, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.05143 to 1.04367, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.04367 to 1.03555, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.03555 to 1.02784, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.02784 to 1.02040, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.02040 to 1.01280, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.01280 to 1.00547, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.00547 to 0.99784, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.99784 to 0.98951, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.98951 to 0.97991, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.97991 to 0.96985, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.96985 to 0.95923, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.95923 to 0.94765, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.94765 to 0.93589, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.93589 to 0.92331, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.92331 to 0.90942, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.90942 to 0.89375, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.89375 to 0.87716, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.87716 to 0.85852, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.85852 to 0.83801, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.83801 to 0.81884, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.81884 to 0.79966, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.79966 to 0.78082, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.78082 to 0.76073, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.76073 to 0.74202, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.74202 to 0.72262, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.72262 to 0.70585, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.70585 to 0.68864, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.68864 to 0.67246, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.67246 to 0.65632, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.65632 to 0.64101, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.64101 to 0.62778, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.62778 to 0.61652, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.61652 to 0.60575, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.60575 to 0.59600, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.59600 to 0.58734, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.58734 to 0.57966, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.57966 to 0.57231, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.57231 to 0.56698, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.56698 to 0.56221, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.56221 to 0.55742, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.55742 to 0.55200, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.55200 to 0.54712, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.54712 to 0.54194, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.54194 to 0.54018, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.54018 to 0.53869, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.53869 to 0.53417, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.53417 to 0.53022, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.53022 to 0.52254, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.52254 to 0.51678, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.51678 to 0.51331, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.51331 to 0.51138, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.51138 to 0.50725, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.50725 to 0.50372, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.50372 to 0.50167, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.50167\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.50167\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.50167\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.50167\n",
      "\n",
      "Epoch 66: val_loss improved from 0.50167 to 0.49862, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.49862 to 0.49169, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.49169 to 0.48163, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.48163 to 0.47562, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.47562 to 0.47405, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.47405\n",
      "\n",
      "Epoch 72: val_loss improved from 0.47405 to 0.47292, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.47292 to 0.46576, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.46576 to 0.45557, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.45557\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.45557\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.45557\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.45557\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.45557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80: val_loss improved from 0.45557 to 0.44460, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.44460 to 0.44117, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.44117 to 0.43726, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.43726 to 0.43141, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.43141 to 0.42090, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.42090 to 0.40512, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.40512 to 0.38866, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.38866 to 0.37697, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.37697 to 0.36840, saving model to /best_model\\model_9.h5\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.36840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 237: val_loss did not improve from 0.36840\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.36840\n",
      "Epoch 238: early stopping\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3684 - accuracy: 0.7778\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.09421, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 1.09421 to 1.09160, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 1.09160 to 1.08808, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 1.08808 to 1.08430, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 1.08430 to 1.08019, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 1.08019 to 1.07552, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 1.07552 to 1.07082, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 1.07082 to 1.06556, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 1.06556 to 1.06027, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 1.06027 to 1.05454, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 1.05454 to 1.04857, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 1.04857 to 1.04221, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 1.04221 to 1.03547, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 1.03547 to 1.02803, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 1.02803 to 1.02017, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 1.02017 to 1.01239, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 1.01239 to 1.00370, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 1.00370 to 0.99424, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.99424 to 0.98400, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98400 to 0.97303, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.97303 to 0.96218, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.96218 to 0.95090, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.95090 to 0.93875, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.93875 to 0.92608, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.92608 to 0.91009, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.91009 to 0.89230, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.89230 to 0.87340, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.87340 to 0.85404, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.85404 to 0.83484, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.83484 to 0.81472, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.81472 to 0.79302, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.79302 to 0.77063, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.77063 to 0.74892, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.74892 to 0.72819, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.72819 to 0.70706, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.70706 to 0.68530, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.68530 to 0.66474, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.66474 to 0.64332, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.64332 to 0.62199, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.62199 to 0.60074, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.60074 to 0.57959, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.57959 to 0.55839, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.55839 to 0.53787, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.53787 to 0.52006, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.52006 to 0.50498, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.50498 to 0.49187, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.49187 to 0.47666, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.47666 to 0.45997, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.45997 to 0.44297, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.44297 to 0.42875, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.42875 to 0.41563, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.41563 to 0.40394, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.40394 to 0.39280, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.39280 to 0.38226, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.38226 to 0.37243, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.37243 to 0.36427, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.36427 to 0.35365, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.35365 to 0.34052, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.34052 to 0.33001, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.33001 to 0.32147, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.32147 to 0.31992, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.31992 to 0.31027, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.31027 to 0.29837, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.29837 to 0.28721, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.28721 to 0.28169, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.28169 to 0.27901, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.27901 to 0.26881, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.26881 to 0.25848, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.25848 to 0.24890, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.24890 to 0.23954, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.23954 to 0.23079, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.23079 to 0.22250, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.22250 to 0.21569, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.21569 to 0.20845, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.20845 to 0.19915, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.19915 to 0.19091, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.19091 to 0.18762, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.18762 to 0.18350, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.18350 to 0.17340, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.17340 to 0.16650, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.16650 to 0.16160, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.16160 to 0.15764, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.15764 to 0.15268, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.15268 to 0.14935, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.14935 to 0.14455, saving model to /best_model\\model_10.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86: val_loss improved from 0.14455 to 0.13678, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.13678 to 0.13041, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.13041 to 0.12341, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.12341 to 0.12001, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.12001 to 0.11482, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.11482 to 0.11426, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.11426\n",
      "\n",
      "Epoch 93: val_loss improved from 0.11426 to 0.10687, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.10687 to 0.09293, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.09293 to 0.08268, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.08268 to 0.07827, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.07827 to 0.07810, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.07810\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.07810\n",
      "\n",
      "Epoch 100: val_loss improved from 0.07810 to 0.05988, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.05988 to 0.05487, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.05487 to 0.05118, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.05118 to 0.04718, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.04718\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.04718\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.04718\n",
      "\n",
      "Epoch 107: val_loss improved from 0.04718 to 0.03959, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.03959 to 0.03652, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.03652 to 0.03370, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.03370 to 0.03226, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.03226 to 0.02904, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.02904 to 0.02704, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.02704 to 0.02557, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.02557 to 0.02427, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.02427 to 0.02267, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.02267 to 0.02118, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.02118 to 0.01993, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.01993 to 0.01873, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.01873\n",
      "\n",
      "Epoch 120: val_loss improved from 0.01873 to 0.01701, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.01701 to 0.01554, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.01554 to 0.01524, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.01524 to 0.01486, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.01486 to 0.01384, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.01384 to 0.01312, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.01312 to 0.01274, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.01274\n",
      "\n",
      "Epoch 129: val_loss improved from 0.01274 to 0.01228, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.01228 to 0.01163, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.01163 to 0.01108, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.01108 to 0.01070, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.01070 to 0.01033, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.01033\n",
      "\n",
      "Epoch 135: val_loss improved from 0.01033 to 0.01024, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.01024\n",
      "\n",
      "Epoch 137: val_loss improved from 0.01024 to 0.00963, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.00963 to 0.00917, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.00917 to 0.00845, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.00845 to 0.00809, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.00809 to 0.00782, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.00782 to 0.00781, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.00781\n",
      "\n",
      "Epoch 155: val_loss improved from 0.00781 to 0.00762, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00762 to 0.00737, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00737\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00737 to 0.00725, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00725 to 0.00716, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.00716 to 0.00714, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.00714\n",
      "\n",
      "Epoch 186: val_loss improved from 0.00714 to 0.00669, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.00669 to 0.00617, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.00617 to 0.00582, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.00582 to 0.00561, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00561 to 0.00545, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.00545 to 0.00534, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.00534 to 0.00520, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.00520\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.00520\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.00520\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.00520\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.00520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 198: val_loss did not improve from 0.00520\n",
      "\n",
      "Epoch 199: val_loss improved from 0.00520 to 0.00489, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.00489 to 0.00444, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.00444 to 0.00402, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.00402 to 0.00380, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.00380 to 0.00364, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.00364 to 0.00351, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.00351 to 0.00338, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.00338 to 0.00323, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.00323 to 0.00310, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.00310 to 0.00298, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.00298 to 0.00292, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.00292 to 0.00291, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 213: val_loss improved from 0.00291 to 0.00291, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.00291 to 0.00286, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 216: val_loss improved from 0.00286 to 0.00276, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.00276 to 0.00265, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.00265 to 0.00263, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.00263 to 0.00258, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 249: val_loss improved from 0.00258 to 0.00252, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.00252\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.00252\n",
      "\n",
      "Epoch 252: val_loss improved from 0.00252 to 0.00244, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.00244 to 0.00235, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.00235 to 0.00228, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.00228 to 0.00221, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.00221 to 0.00215, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.00215 to 0.00214, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 263: val_loss improved from 0.00214 to 0.00200, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.00200 to 0.00180, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.00180 to 0.00167, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.00167 to 0.00156, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.00156 to 0.00149, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.00149 to 0.00146, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.00146 to 0.00142, saving model to /best_model\\model_10.h5\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.00142\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Wall time: 10min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 앞에 keras 관련 import문이랑 사용자정의함수 돌리고 실행하기\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix # 혼동행렬\n",
    "\n",
    "k = 10\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "acc_score = [] # 정확도를 정리할 리스트\n",
    "cf_matrix = [] # 혼동행렬을 정리할 리스트\n",
    "\n",
    "\n",
    "fold_val = 1\n",
    "for train_index, test_index in kfold.split(curve_x, y.argmax(axis=1)) : # ****************\n",
    "    curve_x_train, curve_x_test = curve_x[train_index], curve_x[test_index] # ****************\n",
    "    pl_x_train, pl_x_test = pl_x[train_index], pl_x[test_index] # ****************\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # create new model\n",
    "    model = model_fn2() \n",
    "    # compile new model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=150) \n",
    "    mc = ModelCheckpoint(get_model_name(fold_val), monitor='val_loss', mode='min',  \n",
    "                         save_best_only=True, verbose = 1)\n",
    "    history = model.fit(x=[curve_x_train, pl_x_train], y=y_train, batch_size=32, epochs=300, verbose=0, \n",
    "                       callbacks=[es, mc], validation_data=([curve_x_test, pl_x_test], y_test)) # ****************\n",
    "    \n",
    "    model.load_weights(\"/best_model/model_\"+str(fold_val)+\".h5\") \n",
    "    \n",
    "    accuracy = model.evaluate([curve_x_test, pl_x_test], y_test)[1] # ****************\n",
    "    acc_score.append(accuracy)\n",
    "    \n",
    "    # 혼동행렬\n",
    "    y_pred = model.predict([curve_x_test, pl_x_test]).argmax(axis=1) # ****************\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    cf_matrix.append(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ea3f996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [1.0, 1.0, 0.8888888955116272, 1.0, 0.9444444179534912]\n",
      "정확도 평균 : 0.9666666626930237\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e8029fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5, 0, 0],\n",
       "        [0, 4, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[5, 0, 0],\n",
       "        [0, 4, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [1, 4, 0],\n",
       "        [0, 1, 8]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [0, 5, 0],\n",
       "        [0, 0, 9]], dtype=int64),\n",
       " array([[4, 0, 0],\n",
       "        [0, 4, 0],\n",
       "        [0, 1, 9]], dtype=int64)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98766be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29bb118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [1.0, 1.0, 0.7777777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777910232544, 1.0]\n",
      "정확도 평균 : 0.9555555582046509\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6075b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[1, 0, 1],\n",
       "        [1, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 3, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[1, 1, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 1, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a381d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "333b7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "정확도 평균 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# k번 계산된 정확도의 평균\n",
    "avg_acc_score = sum(acc_score) / k\n",
    "\n",
    "print('정확도 :', acc_score)\n",
    "print('정확도 평균 :', avg_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77354c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[3, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 3, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 3, 0],\n",
       "        [0, 0, 4]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64),\n",
       " array([[2, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 5]], dtype=int64)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41100c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python2021] *",
   "language": "python",
   "name": "conda-env-python2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
